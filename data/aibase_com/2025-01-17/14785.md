# 腾讯混元发布浮点量化训练新理论，揭示大模型训练的极限

**发布日期**: 2025年1月17号 9:32

![新闻图片](https://pic.chinaz.com/picmap/thumb/202307261637362209_5.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14785)

## 内容

在大模型（Large Language Model，LLM）快速发展的今天，模型的训练和推理成本日益成为研究和应用的关注焦点。最近，腾讯混元团队发布了一项重要研究，深入探讨了低比特浮点量化训练的 “Scaling Laws”，即浮点数量化训练的规模法则。此项研究的核心在于通过降低模型的精度，探索如何在不损失性能的情况下，显著降低计算和存储成本。研究团队进行了多达366组不同参数规模和精度的浮点数量化训练，系统分析了影响训练效果的多种因素，包括模型大小（N）、训练数据量(D)、指数位(E)、尾数位(M)以及量化粒度(B)。通过这些实验，研究人员得出了一套统一的 Scaling Law，揭示了在不同精度下，如何有效配置训练数据和模型参数，以获得最佳的训练效果。最为关键的是，研究指出，在任意低精度的浮点数量化训练中，存在一个 “极限效果”，即在特定的数据量下，模型的性能将达到最优，超过此数据量可能会导致效果下降。此外，研究还发现，理论上最佳性价比的浮点数量化训练精度应在4到8比特之间，这对于开发高效的 LLM 具有重要的指导意义。该研究不仅填补了浮点数量化训练领域的空白，也为未来硬件制造商提供了参考，帮助他们在不同精度下优化浮点运算能力。最终，这项研究为大模型训练的实践提供了清晰的方向，确保在资源有限的情况下，依然能够实现高效的训练效果。论文地址：https://arxiv.org/pdf/2501.02423
