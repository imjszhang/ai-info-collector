# 香港大学与快手科技联合推出 GameFactory 框架，助力游戏视频生成创新

**发布日期**: 2025年1月20号 9:33

![新闻图片](https://upload.chinaz.com/2025/0120/6387296240335072676358349.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14828)

## 内容

在游戏开发领域，场景的多样性和创新性一直是一个难题。近期，香港大学与快手科技联手研发了一个名为 GameFactory 的创新框架，旨在解决游戏视频生成中的场景泛化问题。这一框架利用了预训练的视频扩散模型，能够在开放域的视频数据上进行训练，从而生成全新且多样化的游戏场景。视频扩散模型作为一种先进的生成技术，近年来在视频生成和物理模拟领域展现出巨大潜力。这些模型能够像视频生成工具一样，响应用户的操作输入，如键盘和鼠标，进而生成相应的游戏画面。然而，场景泛化，指的是超越现有游戏场景创造全新游戏场景的能力，仍然是这一领域的重大挑战。虽然大量收集动作标注的视频数据集是解决这一问题的直接方法，但这种方法耗时耗力，尤其是在开放域场景中更显不切实际。GameFactory 框架的推出正是为了解决这一难题。通过预训练的视频扩散模型，GameFactory 能够避免对特定游戏数据集的过度依赖，并且支持生成多样化的游戏场景。此外，为了弥补开放域先验知识与有限游戏数据集之间的差距，GameFactory 还采用了独特的三阶段训练策略。在第一阶段，利用 LoRA（低秩适配）微调预训练模型，使其适应特定的游戏领域，并保留原始参数。第二阶段则冻结预训练参数，专注于训练动作控制模块，以避免风格和控制的混淆。最后，在第三阶段中，移除 LoRA 权重，保留动作控制模块参数，使系统能够在不同的开放域场景中生成受控的游戏视频。研究人员还评估了不同控制机制的有效性，发现交叉注意力机制在处理键盘输入等离散控制信号时表现更优，而拼接方法在处理鼠标移动信号时效果更佳。GameFactory 还支持自回归动作控制，能够生成无限长度的交互式游戏视频。此外，研究团队还发布了高质量的动作标注视频数据集 GF-Minecraft，以供框架的训练和评估使用。论文:https://arxiv.org/abs/2501.08325划重点:🌟 GameFactory 框架由香港大学与快手科技联合研发，旨在解决游戏视频生成中的场景泛化问题。🎮 该框架利用预训练的视频扩散模型，能够生成多样化的游戏场景，并采用三阶段训练策略提升效果。📊 研究人员还发布了动作标注视频数据集 GF-Minecraft，以支持 GameFactory 的训练和评估。
