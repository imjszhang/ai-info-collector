# ​北大团队发布多模态模型 LLaVA-o1，推理能力堪比 GPT-o1！

**发布日期**: 2024年11月19号 13:51

![新闻图片](https://upload.chinaz.com/2024/1119/6386762088836954344127546.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13322)

## 内容

近日，北京大学等科研团队宣布发布了一款名为 LLaVA-o1的多模态开源模型，据称这是首个能够进行自发、系统推理的视觉语言模型，堪比 GPT-o1。该模型在六个具有挑战性的多模态基准测试中表现优异，其11B 参数的版本超越了其他竞争者，如 Gemini-1.5-pro、GPT-4o-mini 和 Llama-3.2-90B-Vision-Instruct。LLaVA-o1基于 Llama-3.2-Vision 模型，采用了 “慢思考” 推理机制，能够自主进行更加复杂的推理过程，超越了传统的思维链提示方法。在多模态推理基准测试中，LLaVA-o1的表现超出了其基础模型8.9%。该模型的独特之处在于其推理过程被分为四个阶段:总结、视觉解释、逻辑推理和结论生成。在传统模型中，推理过程往往比较简单，容易导致错误答案，而 LLaVA-o1通过结构化的多步骤推理，确保了更为精准的输出。例如，在解决 “减去所有的小亮球和紫色物体，剩下多少个物体?” 的问题时，LLaVA-o1会首先总结问题，接着从图像中提取信息，然后进行逐步推理，最终给出答案。这种分阶段的方法提升了模型的系统推理能力，使其在处理复杂问题时更为高效。值得一提的是，LLaVA-o1在推理过程中引入了阶段级光束搜索方法。这种方法允许模型在每个推理阶段生成多个候选答案，并选择最佳的答案继续进行下一阶段的推理，从而显著提高了整体推理质量。通过监督微调和合理的训练数据，LLaVA-o1在与更大或闭源模型的比较中表现出色。北大团队的研究成果不仅推动了多模态 AI 的发展，也为未来的视觉语言理解模型提供了新的思路和方法。团队表示，LLaVA-o1的代码、预训练权重和数据集都将全面开源，期待更多研究者和开发者能够共同探索和应用这一创新模型。论文:https://arxiv.org/abs/2411.10440GitHub:https://github.com/PKU-YuanGroup/LLaVA-o1划重点:🌟 LLaVA-o1是北京大学等团队发布的全新多模态推理模型，具备 “慢思考” 推理能力。📈 该模型在多模态推理基准测试中性能超越基础模型8.9%。🔍 LLaVA-o1通过结构化的多步骤推理，确保准确性，并将于近期开源。
