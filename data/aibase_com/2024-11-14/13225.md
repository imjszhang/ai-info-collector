# Exo Labs登场！让Mac M4电脑也能本地运行强大开源AI模型

**发布日期**: 2024年11月14号 10:11

![新闻图片](https://upload.chinaz.com/2024/1114/6386717586419130324581976.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13225)

## 内容

在生成式人工智能领域，苹果的努力似乎主要集中在移动设备上，尤其是最新的 iOS18系统。然而，新的 Apple M4芯片在最新发布的 Mac Mini 和 Macbook Pro 中展现出强大的性能，使其能够有效运行目前最强大的开源基础大语言模型（LLMs），如 Meta 的 Llama-3.1405B、Nvidia 的 Nemotron70B 和 Qwen2.5Coder-32B。Exo Labs 是一家成立于2024年3月的初创公司，致力于 “民主化人工智能的访问”，其联合创始人亚历克斯・奇马（Alex Cheema）已经成功地利用多台 M4设备搭建了一个本地计算集群。他将四台 Mac Mini M4（每台售价599美元）与一台 Macbook Pro M4Max(售价1599美元)相连，通过 Exo 的开源软件运行了阿里巴巴的 Qwen2.5Coder-32B。整个集群的成本大约为5000美元，相较于一台价值25000到30000美元的 Nvidia H100GPU 来说，性价比极高。使用本地计算集群而非网络服务的好处显而易见。通过在用户或企业控制的设备上运行 AI 模型，可以有效降低成本，同时提升隐私和安全性。奇马表示，Exo Labs 正在不断完善其企业级软件，目前已有几家公司在使用 Exo 软件进行本地 AI 推理，未来这一趋势将逐步向个人和企业扩展。Exo Labs 近期的成功得益于 M4芯片的强大性能，该芯片被称为 “全球最快的 GPU 核心”。奇马透露，Exo Labs 的 Mac Mini M4集群能够以每秒18个标记的速度运行 Qwen2.5Coder32B，并以每秒8个标记的速度运行 Nemotron-70B。这表明，用户无需依赖云基础设施就能高效处理 AI 训练和推理任务，使 AI 对隐私和成本敏感的消费者和企业变得更加可及。为了进一步支持这一本地 AI 创新的浪潮，Exo Labs 计划推出一个免费的基准测试网站，以提供详细的硬件配置比较，帮助用户根据需求和预算选择最佳的 LLM 运行解决方案。项目入口:https://github.com/exo-explore/exo划重点:🌟 Exo Labs 利用 Apple M4芯片，成功在本地计算集群上运行强大的开源 AI 模型。💰 本地运行 AI 模型可降低成本，提高隐私安全，避免对云服务的依赖。📊 Exo Labs 将推出基准测试网站，帮助用户选择适合的硬件配置进行 AI 任务。
