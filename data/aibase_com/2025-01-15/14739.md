# 微软AI安全报告揭示：最有效的攻击源自“快速工程”而非复杂技术

**发布日期**: 2025年1月15号 14:59

![新闻图片](https://pic.chinaz.com/picmap/thumb/202005261133548298_13.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14739)

## 内容

自2021年以来，微软的 AI 安全团队对100多种生成式 AI 产品进行了测试，以寻找薄弱环节和道德问题。他们的发现挑战了一些关于 AI 安全的常见假设，并强调了人类专业知识的持续重要性。事实证明，最有效的攻击并不总是最复杂的攻击。微软报告中引用的一项研究指出:“真正的黑客不会计算梯度，而是使用快速工程。”该研究将人工智能安全研究与现实世界的实践进行了比较。在一次测试中，该团队仅通过将有害指令隐藏在图像文本中就成功绕过了图像生成器的安全功能——无需复杂的数学运算。人情味依然重要虽然微软已经开发了 PyRIT，一种可以自动进行安全测试的开源工具，但该团队强调，人类的判断力是无法被取代的。当他们测试聊天机器人如何处理敏感情况（例如与情绪困扰的人交谈）时，这一点变得尤为明显。评估这些场景既需要心理学专业知识，也需要对潜在心理健康影响的深刻理解。在调查人工智能偏见时，该团队还依赖人类的洞察力。在一个例子中，他们通过创建不同职业的图片（不指定性别）来检查图像生成器中的性别偏见。新的安全挑战出现人工智能与日常应用的融合带来了新的漏洞。在一次测试中，该团队成功操纵语言模型，创造出令人信服的欺诈场景。当与文本转语音技术相结合时，这就创建了一个可以以危险的逼真方式与人互动的系统。风险并不局限于人工智能特有的问题。该团队在一款人工智能视频处理工具中发现了一个传统的安全漏洞（SSRF），表明这些系统面临着新旧安全挑战。持续的安全需求这项研究特别关注“负责任的人工智能”风险，即人工智能系统可能生成有害或有道德问题的内容的情况。这些问题特别难以解决，因为它们通常严重依赖于背景和个人解读。微软团队发现，普通用户无意接触有问题的内容比故意攻击更令人担忧，因为这表明安全措施在正常使用过程中没有按预期发挥作用。研究结果清楚地表明，人工智能安全并非一次性解决。微软建议持续寻找和修复漏洞，然后进行更多测试。他们建议，这需要有法规和财务激励措施的支持，使成功的攻击更加昂贵。研究团队表示，仍有几个关键问题需要解决:我们如何识别和控制具有潜在危险的人工智能能力，如说服和欺骗?我们如何根据不同的语言和文化调整安全测试?公司如何以标准化的方式分享他们的方法和结果?
