# ​MiniMax开源MiniMax-01全新系列模型 性能比肩GPT-4o

**发布日期**: 2025年1月15号 9:21

![新闻图片](https://pic.chinaz.com/thumb/2025/0115/25011509212868980986.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14712)

## 内容

MiniMax于2025年1月15日宣布开源其全新系列模型MiniMax-01，该系列包含基础语言大模型MiniMax-Text-01和视觉多模态大模型MiniMax-VL-01。MiniMax-01系列在架构上进行了大胆创新，首次大规模实现线性注意力机制，打破了传统Transformer架构的局限。其参数量高达4560亿，单次激活459亿，综合性能与海外顶尖模型相当，且能高效处理长达400万token的上下文，这一长度是GPT-4o的32倍、Claude-3.5-Sonnet的20倍。MiniMax认为2025年将是Agent高速发展的关键年份，无论是单Agent系统还是多Agent系统，都需要更长的上下文来支持持续记忆和大量通信。MiniMax-01系列模型的推出，正是为了满足这一需求，迈出建立复杂Agent基础能力的第一步。得益于架构创新、效率优化以及集群训推一体设计，MiniMax能够以业内最低的价格区间提供文本和多模态理解的API服务，标准定价为输入token1元/百万token，输出token8元/百万token。MiniMax开放平台及海外版已上线，供开发者体验使用。MiniMax-01系列模型已在GitHub开源，并将持续更新。在业界主流的文本和多模态理解测评中，MiniMax-01系列在多数任务上追平了海外公认的先进模型GPT-4o-1120和Claude-3.5-Sonnet-1022。特别是在长文任务上，与Google的Gemini模型相比，MiniMax-Text-01随着输入长度增加，性能衰减最慢，显著优于Gemini。MiniMax的模型在处理长输入时效率极高，接近线性复杂度。其结构设计中，每8层中有7层采用基于Lightning Attention的线性注意力，1层采用传统SoftMax注意力。这是业内首次将线性注意力机制扩展到商用模型级别，MiniMax在Scaling Law、与MoE结合、结构设计、训练优化和推理优化等方面进行了综合考量，并重构了训练和推理系统，包括更高效的MoE All-to-all通讯优化、更长序列优化以及推理层面线性注意力的高效Kernel实现。在大部分学术集上，MiniMax-01系列取得了比肩海外第一梯队的结果。在长上下文测评集上更是显著领先，如在400万的Needle-In-A-Haystack检索任务上表现优异。除了学术数据集，MiniMax还构建了基于真实数据的助手场景测试集，MiniMax-Text-01在该场景中表现突出。在多模态理解测试集中，MiniMax-VL-01也较为领先。开源地址：https://github.com/MiniMax-AI
