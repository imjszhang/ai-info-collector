# DeepSeek推出全新推理模型 DeepSeek-R1 性能比肩OpenAI-o1

**发布日期**: 2025年1月21号 8:23

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14865)

## 内容

2025年1月20日，DeepSeek 宣布推出其首个通过强化学习 （RL） 训练的推理模型 DeepSeek-R1，该模型在多个推理基准测试中取得了与 OpenAI-o1-1217相当的性能。DeepSeek-R1基于 DeepSeek-V3-Base 模型，并采用了多阶段训练和冷启动数据来提高推理能力。DeepSeek 的研究人员首先开发了 DeepSeek-R1-Zero，这是一个完全通过大规模强化学习训练的模型，没有任何监督微调的预备步骤。DeepSeek-R1-Zero 在推理基准测试中展现出卓越的性能，例如在 AIME2024考试中，其 pass@1分数从15.6% 提升至71.0%。然而，DeepSeek-R1-Zero 也存在一些问题，例如可读性差和语言混杂。为了解决这些问题并进一步提升推理性能，DeepSeek 团队开发了 DeepSeek-R1。DeepSeek-R1在强化学习之前引入了多阶段训练和冷启动数据。 具体而言，研究人员首先收集了数千个冷启动数据对 DeepSeek-V3-Base 模型进行微调。 然后，他们像训练 DeepSeek-R1-Zero 一样进行了面向推理的强化学习。 在强化学习过程接近收敛时，他们通过对强化学习检查点进行拒绝抽样创建了新的监督微调数据，并结合 DeepSeek-V3在写作、事实问答和自我认知等领域中的监督数据，然后重新训练 DeepSeek-V3-Base 模型。 最后，使用所有场景的提示对微调后的检查点进行额外的强化学习。DeepSeek-R1在多个基准测试中取得了令人瞩目的成绩:•在 AIME2024考试中，DeepSeek-R1的 pass@1分数达到了79.8%，略高于 OpenAI-o1-1217。•在 MATH-500考试中，DeepSeek-R1的 pass@1分数达到了97.3%，与 OpenAI-o1-1217持平。•在代码竞赛任务中，DeepSeek-R1在 Codeforces 上获得了2029的 Elo 评级，超过了96.3% 的人类参赛者。•在知识基准测试（如 MMLU、MMLU-Pro 和 GPQA Diamond）中，DeepSeek-R1的得分分别为90.8%、84.0% 和71.5%，显著优于 DeepSeek-V3。•在其他任务（如创意写作、一般问答、编辑、摘要等）中，DeepSeek-R1也表现出色。此外，DeepSeek 还探索了将 DeepSeek-R1的推理能力蒸馏到更小的模型中。研究发现，直接从 DeepSeek-R1进行蒸馏比在小型模型上应用强化学习的效果更好。 这表明大型基础模型发现的推理模式对于提高推理能力至关重要。DeepSeek 已开源了 DeepSeek-R1-Zero、DeepSeek-R1以及基于 Qwen 和 Llama 的六个从 DeepSeek-R1蒸馏的密集模型（1.5B、7B、8B、14B、32B、70B）。 DeepSeek-R1的推出，标志着强化学习在提高大型语言模型推理能力方面取得了重大进展。成本优势在成本方面，DeepSeek-R1提供了极具竞争力的定价策略。其 API 访问定价为每百万输入令牌0.14美元（缓存命中）和0.55美元(缓存未命中)，输出令牌每百万2.19美元。这一价格策略相比其他同类产品更具吸引力，被用户形容为“游戏规则改变者”。目前官方网站和 API 现已上线！访问 https://chat.deepseek.com 就可以体验 DeepThink!社区反馈与未来展望DeepSeek-R1的发布引发了社区的热烈讨论。许多用户对模型的开源特性和成本优势表示赞赏，认为其为开发者提供了更多的选择和自由。然而，也有用户对模型的上下文窗口大小提出疑问，希望未来版本能够进一步优化。DeepSeek 团队表示，他们将继续致力于提升模型的性能和用户体验，同时计划在未来推出更多功能，包括高级数据分析，以满足用户对 AGI（通用人工智能）的期待。
