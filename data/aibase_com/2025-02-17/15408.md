# Meta 创新推出 “连续概念混合” 框架，推动 Transformer 预训练新革命

**发布日期**: 2025年2月17号 10:05

![新闻图片](https://pic.chinaz.com/picmap/thumb/202111072153100579_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15408)

## 内容

近年来，随着大型语言模型（LLMs）的快速发展，自然语言处理领域经历了前所未有的变革。这些技术如今广泛应用于代码助手、搜索引擎和个人 AI 助手等场景，展现了强大的能力。然而，传统的 “下一个 token 预测” 范式存在一定局限性，尤其是在处理复杂推理和长期任务时，模型需要经历大量训练才能掌握深层次的概念理解。为了解决这一问题，Meta 等机构的研究者们提出了一种名为 “连续概念混合”（CoCoMix）的新颖预训练框架。这一方法不仅保留了下一个 token 预测的优点，还引入了通过稀疏自编码器(SAE)学习到的连续概念，从而提升模型的学习效率和表现。具体来说，CoCoMix 通过选择最具影响力的概念，将其与 token 的隐藏表示交错结合，形成了一个全新的学习机制。在实际应用中，研究者对 CoCoMix 进行了广泛评估，涵盖了多个语言建模基准和不同规模的模型。结果显示，CoCoMix 在训练 token 的数量减少21.5% 的同时，仍然能够达到与传统 token 预测相当的性能。这一发现令人振奋，尤其在从小模型中提取概念用于指导大模型的弱到强监督场景中，CoCoMix 展现出了显著的改进。此外，CoCoMix 的可解释性和可操控性也成为其重要特征之一。研究者通过观察模型在预测过程中的表现，可以清楚地了解模型重点关注哪些概念，并通过调整概念的大小来操控模型的输出结果。这一特性为进一步的模型分析和优化提供了新的视角。总的来说，CoCoMix 不仅是对现有语言模型训练方式的一次创新，也是 Meta 在引领大模型发展趋势方面的一次重要尝试。随着技术的不断进步，这一框架或将成为未来自然语言处理领域的关键工具，推动 AI 更智能的演进。项目地址：https://github.com/facebookresearch/RAM/tree/main/projects/cocomix
