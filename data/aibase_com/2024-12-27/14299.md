# 挑战开源AI新高度:DeepSeek V3超越Llama3.1，训练数据达14.8万亿token

**发布日期**: 2024年12月27号 1:42

![新闻图片](https://pic.chinaz.com/picmap/202304251756308111_1.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14299)

## 内容

中国人工智能公司DeepSeek日前发布了一款具有里程碑意义的开源大语言模型DeepSeek V3。这款拥有6710亿参数的模型不仅规模超越Meta的Llama3.1，在多项基准测试中的表现也优于包括GPT-4在内的主流封闭源模型。DeepSeek V3的突出特点在于其强大的性能与高效的开发过程。该模型在编程平台Codeforces的竞赛中表现出色，并在测试代码集成能力的Aider Polyglot测试中领先竞争对手。模型训练采用了14.8万亿token的庞大数据集，参数规模达到了Llama3.1的1.6倍。更引人注目的是，DeepSeek仅用两个月时间、550万美元成本就完成了模型训练，这个数字远低于同类产品的开发投入。DeepSeek背后的支持方是中国量化对冲基金High-Flyer Capital Management。该基金投资建设了拥有10，000个Nvidia A100GPU、价值约1.38亿美元的服务器集群。High-Flyer创始人梁文峰表示，开源AI终将打破当前封闭模型的垄断优势。DeepSeek V3采用宽松许可证发布，允许开发者下载、修改并将其用于包括商业用途在内的各类应用。尽管运行完整版本仍需要强大的硬件支持，但这一开源模型的发布标志着AI领域的开放创新迈出重要一步。
