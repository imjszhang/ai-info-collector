# 书生·万象多模态大模型InternVL 2.5开源 性能媲美GPT-4o

**发布日期**: 2024年12月10号 0:16

![新闻图片](https://upload.chinaz.com/2024/1210/6386941527557482516679167.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13797)

## 内容

近日，上海 AI 实验室宣布推出书生·万象InternVL2.5模型。这款开源多模态大型语言模型以其卓越的性能，成为首个在多模态理解基准(MMMU)上超过70%准确率的开源模型，与商业模型如GPT-4o和Claude-3.5-Sonnet等的性能相媲美。InternVL2.5模型通过链式思考（CoT）推理技术实现了3.7个百分点的提升，展现了强大的测试时间可扩展性潜力。该模型基于InternVL2.0进一步发展，通过增强训练和测试策略以及提高数据质量来进一步提升性能。在视觉编码器、语言模型、数据集大小和测试时间配置等方面进行了深入研究，以探索模型规模与性能之间的关系。InternVL2.5在多项基准测试中展现了竞争性的性能，特别是在多学科推理、文档理解、多图像/视频理解、现实世界理解、多模态幻觉检测、视觉地面化、多语言能力以及纯语言处理等领域。这一成果不仅为开放源代码社区提供了一个新标准，用于开发和应用多模态AI系统，也为人工智能领域的研究和应用开辟了新的可能性。InternVL2.5保留了其前身InternVL1.5和InternVL2.0的相同模型架构，遵循“ViT-MLP-LLM”范式，并实现了将新的增量预训练的InternViT-6B或InternViT-300M与各种不同大小和类型的预先训练的LLMs集成在一起，使用随机初始化的两层MLP投影器。为了增强高分辨率处理的可扩展性，研究团队应用了一个像素无序操作，将视觉令牌的数量减少到原始数量的一半。模型的开源性质意味着研究人员和开发者可以自由访问和使用InternVL2.5，这将极大地促进多模态AI技术的发展和创新。模型链接:https://www.modelscope.cn/collections/InternVL-25-fbde6e47302942
