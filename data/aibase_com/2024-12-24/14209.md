# AI医学推理能力超越人类医生？哈佛、斯坦福：o1-preview 模型诊断准确率高达80%

**发布日期**: 2024年12月24号 2:18

![新闻图片](https://pic.chinaz.com/picmap/thumb/202307181418301728_3.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14209)

## 内容

人工智能在医疗领域的应用再次迎来重大突破!一项由哈佛大学、斯坦福大学等多所顶尖机构联合开展的研究显示，OpenAI 的 o1-preview 模型在多项医学推理任务中表现出惊人的能力，甚至超越了人类医生。这项研究不仅评估了该模型在医学多项选择题基准测试中的表现，更着重考察了其在模拟真实临床场景下的诊断和管理能力，结果令人瞩目。研究人员通过五个实验，对 o1-preview 模型进行了全面评估，包括鉴别诊断生成、展示诊断推理过程、分诊鉴别诊断、概率推理和管理推理。这些实验均由医学专家使用已验证的心理测量学方法进行评估，旨在将 o1-preview 的表现与此前人类对照组和早期大型语言模型基准进行对比。结果显示，o1-preview 在鉴别诊断生成以及诊断和管理推理的质量方面取得了显著进步。在评估 o1-preview 生成鉴别诊断的能力时，研究人员使用了《新英格兰医学杂志》（NEJM）发布的临床病理讨论会(CPC)病例。结果显示，该模型在78.3% 的病例中给出的鉴别诊断包含了正确诊断，在52% 的病例中，首个诊断即为正确诊断。更为惊人的是，o1-preview 在88.6% 的病例中给出了准确或非常接近的诊断，而之前的 GPT-4模型在相同病例中的这一比例为72.9%。此外，o1-preview 在选择下一步诊断测试方面也表现出色，在87.5% 的病例中选择了正确的测试，11% 的病例中选择的测试方案被认为是有帮助的。为了进一步评估 o1-preview 的临床推理能力，研究人员使用了 NEJM Healer 课程中的20个临床病例。结果显示，o1-preview 在这些病例中的表现明显优于 GPT-4、主治医生和住院医师，在78/80的案例中获得了完美的 R-IDEA 评分。R-IDEA 评分是一个用于评估临床推理记录质量的10分制量表。此外，研究人员还通过 “Grey Matters” 管理案例和 “Landmark” 诊断案例评估了 o1-preview 的管理和诊断推理能力。在 “Grey Matters” 案例中，o1-preview 的得分显著高于 GPT-4、使用 GPT-4的医生和使用传统资源的医生。在 “Landmark” 案例中，o1-preview 的表现与 GPT-4相当，但优于使用 GPT-4或传统资源的医生。然而，研究也发现 o1-preview 在概率推理方面的表现与之前的模型相似，并未取得明显改进。在某些情况下，该模型在预测疾病概率时不如人类。研究人员还指出，o1-preview 的一个局限是倾向于冗长，这可能在一定程度上提高了其在某些实验中的得分。此外，该研究主要关注模型性能，而未涉及人机交互，因此未来需要进一步研究 o1-preview 如何增强人机交互，以开发更有效的临床决策支持工具。尽管如此，这项研究仍表明，o1-preview 在需要复杂批判性思维的任务（如诊断和管理）中表现出色。研究人员强调，医学领域的诊断推理基准正在迅速饱和，因此需要开发更具挑战性和现实性的评估方法。他们呼吁在真实临床环境中对这些技术进行试验，并为临床医生与人工智能的协作创新做好准备。此外，还需建立健全的监督框架，以监控人工智能临床决策支持系统的广泛实施。论文地址：https://www.arxiv.org/pdf/2412.10849
