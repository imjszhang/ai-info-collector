# 多模态模型新突破：李飞飞团队统一动作与语言，不仅超懂指令，还能读懂隐含情绪

**发布日期**: 2024年12月18号 9:52

![新闻图片](https://pic.chinaz.com/picmap/thumb/202310270933190076_7.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14083)

## 内容

李飞飞团队推出了一种新的多模态模型，该模型能够理解和生成人类的动作，并通过结合语言模型，实现了对口头和非口头语言的统一处理。 这一突破性的研究，使得机器不仅可以理解人类的指令，还能读懂动作中蕴含的情绪，从而更自然地进行人机交互。该模型的核心在于其多模态语言模型框架，能够接收音频、动作和文本等多种形式的输入，并输出所需的模态数据。 结合生成式预训练策略，该模型在多个任务上展现出卓越的性能。 例如，在协同语音手势生成方面，该模型不仅超越了现有技术水平，还显著减少了训练所需的数据量。 此外，该模型还解锁了新的应用场景，如可编辑的手势生成以及通过动作预测情绪.人类交流本质上是多模态的，包括言语和非言语线索，如语音、面部表情和身体姿势。 此模型能够理解这些多模态行为，对于创建在游戏、电影和虚拟现实等应用中自然交流的虚拟角色至关重要。 然而，现有的动作生成模型通常仅限于特定的输入模态（语音、文本或动作数据），无法充分利用可用数据的多样性。该模型利用语言模型统一口头和非口头语言，主要有三个原因:语言模型自然地连接不同的模态。语音具有高度语义性，而建模诸如对笑话的反应等任务需要强大的语义推理能力。语言模型通过广泛的预训练获得了强大的语义理解能力。为了实现这一目标，研究团队首先将身体划分为不同的部分（面部、手部、上半身、下半身），并单独对每个部分进行动作标记。 结合文本和语音的标记器，任何模态的输入都可以表示为一系列的标记，供语言模型使用。 该模型采用了两阶段训练流程:首先进行预训练，以实现各种模态与组合身体动作的对齐，以及音频和文本的对齐。 之后，将下游任务转化为指令，并在这些指令上训练模型，使其能够遵循各种任务指令。该模型在 BEATv2协同语音手势生成基准测试中表现出色，远超现有模型。 预训练策略的效果也得到了验证，尤其是在数据稀缺的情况下，表现出强大的泛化能力。 通过在语音-动作和文本-动作任务上进行后训练，模型不仅可以遵循音频和文本提示，还可以实现从动作数据中预测情绪等新功能。在技术细节上，该模型采用模态特定的标记器处理各种输入模态。 具体来说，该模型训练了一个组合身体运动 VQ-VAE，将面部、手部、上半身和下半身的动作转化为离散的标记。 这些模态特定的词汇（音频和文本）被合并到一个统一的多模态词汇表中。 在训练过程中，不同模态的混合标记被用作输入，并通过编码器-解码器语言模型生成输出。该模型还利用多模态词汇表，将不同的模态数据转换为统一的格式进行处理。 在预训练阶段，模型通过执行模态间的转换任务，来学习不同模态之间的对应关系。 例如，模型可以学习将上身动作转换为下身动作，或者将音频转换为文本。 此外，模型还会通过随机屏蔽某些动作帧来学习动作的时间演变。在后训练阶段，模型使用配对数据进行微调，以执行协同语音手势生成或文本到动作生成等下游任务。 为了使模型能够遵循自然的人类指令，研究人员构建了一个多任务指令遵循模板，将音频到动作、文本到动作和情绪到动作等任务转化为指令。 该模型还具备编辑手势的能力，可以根据文本和音频提示生成协同的全身体动作。最后，该模型还解锁了从动作预测情绪的新能力。 这对于心理健康或精神病学等领域具有重要意义。 与其他模型相比，该模型能够更准确地预测动作中表达的情绪，显示出强大的身体语言理解能力。该研究表明，统一人类动作的口头和非口头语言对于实际应用至关重要，而语言模型为此提供了一个强大的框架。论文地址：https://arxiv.org/pdf/2412.10523v1
