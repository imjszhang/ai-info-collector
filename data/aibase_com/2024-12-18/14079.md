# ​谷歌放宽限制:人类监督下可在高风险领域使用生成式 AI

**发布日期**: 2024年12月18号 8:53

![新闻图片](https://pic.chinaz.com/picmap/202307181418301728_3.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14079)

## 内容

谷歌近期更新了其生成式 AI 的使用条款，明确允许客户在 “高风险” 领域，比如医疗和就业，使用其生成式 AI 工具进行 “自动决策”，前提是需要有人工监督。这一变化在公司最新发布的生成式 AI 禁止使用政策中得以体现。图源备注:图片由AI生成，图片授权服务商Midjourney根据更新后的政策，客户可以在监督的情况下，利用谷歌的生成式 AI 做出可能对个人权利产生 “重大不利影响” 的自动决策。这些高风险领域包括就业、住房、保险、社会福利等。之前的条款似乎对高风险的自动决策做出了全面禁止的规定，但谷歌公司表示，其实从一开始就允许在有人的监督下使用生成式 AI 进行此类决策。谷歌发言人回应媒体时表示:“人类监督的要求在我们的政策中一直存在，涵盖所有高风险领域。我们只是重新分类了一些条款，并更明确地列举了一些例子，以便用户理解。”与谷歌的做法相比，谷歌的主要竞争对手如 OpenAI 和 Anthropic 在高风险自动决策方面有更严格的规定。OpenAI 禁止使用其服务进行与信用、就业、住房、教育、社会评分和保险相关的自动决策。而 Anthropic 允许其 AI 在法律、保险、医疗等高风险领域进行自动决策，但仅在 “合格专业人士” 的监督下，并要求客户明确告知其使用 AI 进行此类决策。关于自动决策的 AI 系统，监管机构对此表示关注，认为这类技术可能导致结果偏见。例如，研究显示，AI 在贷款和抵押申请审批中可能会延续历史歧视。人权观察等非营利组织特别呼吁禁止 “社会评分” 系统，认为这威胁到人们获取社会保障的机会，并可能侵犯隐私，进行有偏见的画像。在欧盟，根据《AI 法案》，高风险 AI 系统，包括涉及个人信用和就业决策的系统，面临最严格的监管。这些系统的提供者必须在数据库中注册，进行质量和风险管理，雇佣人类监督者，并向相关部门报告事件等。在美国，科罗拉多州最近通过了一项法律，要求 AI 开发者披露关于 “高风险” AI 系统的信息，并发布系统能力和局限性的摘要。与此同时，纽约市禁止雇主使用自动工具筛选候选人，除非该工具在过去一年内经过偏见审计。划重点:🌟 谷歌允许在高风险领域使用生成式 AI，但需人工监督。🛡️ 其他 AI 公司如 OpenAI 和 Anthropic 对高风险决策有更严格限制。⚖️ 各国监管机构对自动决策的 AI 系统进行审查，以防结果偏见。
