# DeepSeek 开源周首日：发布大模型加速利器FlashMLA 解码性能飙升至3000GB/s

**发布日期**: 2025年2月24号 10:18

![新闻图片](https://pic.chinaz.com/picmap/thumb/202502051542238080_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15641)

## 内容

DeepSeek 开源周首日正式开源其最新技术成果FlashMLA，这是一款专为英伟达Hopper架构GPU打造的高效多层注意力（Multi-Layer Attention）解码内核。该技术特别针对变长序列场景进行优化，可显著提升大模型推理性能。FlashMLA的核心技术特性包括对BF16精度的全面支持，以及采用块大小为64的页式键值缓存（Paged KV Cache）系统，实现更精确的内存管理。在性能表现方面，基于CUDA12.6平台，FlashMLA在H800SXM5GPU上创下了显著成绩:在内存受限场景下达到3000GB/s的处理速度，在计算受限场景下则实现580TFLOPS的算力水平。该项目已经过生产环境验证，展现出优异的稳定性。开发团队表示，FlashMLA的设计借鉴了FlashAttention2&3和cutlass等项目的优秀经验，并在此基础上实现了创新突破。开发者可通过简单的安装命令快速部署FlashMLA:只需执行"python setup.py install"即可完成安装，随后可运行测试脚本"python tests/test_flash_mla.py"体验其性能。开源地址：https://github.com/deepseek-ai/FlashMLA
