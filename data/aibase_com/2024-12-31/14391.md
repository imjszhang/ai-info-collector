# 字节推1.58位量化FLUX模型 内存减少7.7倍，性能不减反增！

**发布日期**: 2024年12月31号 7:31

![新闻图片](https://upload.chinaz.com/2024/1231/6387125575895925679163198.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14391)

## 内容

人工智能（AI）驱动的文本到图像(T2I)生成模型，如DALLE3、Adobe Firefly3等，展现出卓越的生成能力，在现实应用中潜力无限。然而，这些模型通常拥有数十亿的参数，对内存要求极高，这给在移动设备等资源受限的平台上部署带来了巨大挑战。为了解决这些难题，ByteDance和POSTECH的研究人员探索了对T2I模型进行极低位量化的技术。在众多先进模型中，FLUX.1-dev因其公开可用性和出色的性能成为研究目标。研究人员通过一种名为1.58位量化的方法，对FLUX模型中的视觉转换器权重进行压缩，使其仅采用 {-1，0， +1} 三个数值。这种量化方法无需访问图像数据，仅依靠FLUX.1-dev模型的自监督即可完成。与BitNet b1.58方法不同，该方法不是从头训练大型语言模型，而是作为一种针对T2I模型的后训练量化解决方案。通过这种方法，模型存储空间减少了7.7倍，因为1.58位权重使用2位有符号整数存储，实现了从16位精度的压缩。为了进一步提高推理效率，研究人员还开发了一个为低位计算优化的定制内核。该内核使 推理内存使用量减少了超过5.1倍，并提高了推理延迟。在GenEval和T2I Compbench基准测试中的评估表明，1.58位FLUX在保持与全精度FLUX模型相当的生成质量的同时，显著提高了计算效率。具体来说，研究人员将FLUX模型中99.5%的视觉转换器参数（总计119亿）量化为1.58位，从而大幅降低了存储需求。实验结果表明，1.58位FLUX在T2I CompBench和GenEval数据集上的性能与原始FLUX模型相当。在推理速度方面，1.58位FLUX在低性能GPU(如L20和A10)上表现出更显著的改进。总而言之，1.58位FLUX的出现，标志着在使高质量T2I模型能够在内存和延迟受限的设备上实际部署方面迈出了重要一步。尽管1.58位FLUX在速度改进和高分辨率图像细节渲染方面仍存在一些局限性，但其在提高模型效率和降低资源消耗方面的巨大潜力，有望为未来的研究提供新的思路。主要改进总结:模型压缩: 模型存储空间减少了7.7倍。内存优化: 推理内存使用量减少了5.1倍以上。性能保持: 在GenEval和T2I Compbench基准测试中，1.58位FLUX保持了与全精度FLUX模型相当的性能。无需图像数据: 量化过程无需访问任何图像数据，仅依赖模型自身的自监督。定制内核: 采用了为低位计算优化的定制内核，提升了推理效率。项目页：https://chenglin-yang.github.io/1.58bit.flux.github.io/论文地址：https://arxiv.org/pdf/2412.18653模型地址：https://huggingface.co/papers/2412.18653
