# OpenAI 推出新 AI 安全方法，可主动推理规则拒绝危险请求！

**发布日期**: 2024年12月31号 1:20

![新闻图片](https://upload.chinaz.com/2024/1231/6387123360577578018231666.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14362)

## 内容

OpenAI 公布了一种新的 AI 安全方法，旨在通过改变 AI 系统处理安全规则的方式来提升其安全性。这种新的 o 系列模型不再仅仅依赖于通过示例学习好与坏行为，而是能够理解并积极推理特定的安全指南。OpenAI 的研究中举了一个例子，当用户试图通过加密文本获取非法活动的指示时，模型成功解码了信息，但拒绝了请求，并具体引用了将要违反的安全规则。这种逐步推理的过程显示了模型如何有效地遵循相关的安全准则。这款 o1模型的训练过程分为三个阶段。首先，模型学习如何提供帮助。接下来，通过监督学习，模型会研究特定的安全指南。最后，模型使用强化学习来实践应用这些规则，这一步骤帮助模型真正理解并内化这些安全指南。在 OpenAI 的测试中，新推出的 o1模型在安全性方面表现显著优于其他主流系统，如 GPT-4o、Claude3.5Sonnet 和 Gemini1.5Pro。测试内容包括模型如何拒绝有害请求并允许合适请求的通过，结果显示 o1模型在准确性和抵御越狱尝试方面均取得了最高分。OpenAI 的联合创始人沃伊切赫・扎伦巴在社交平台上表示，他对这种 “深思熟虑的对齐” 工作感到非常自豪，认为这种推理模型可以以一种全新的方式进行对齐，特别是在发展人工通用智能（AGI）时，确保系统与人类价值观保持一致是一项重大挑战。尽管 OpenAI 声称取得了进展，然而名为 “解放者普林尼” 的黑客仍然展示了即便是新的 o1和 o1-Pro 模型也能被操纵以突破安全指南。普林尼成功让模型生成成人内容，甚至分享制作莫洛托夫鸡尾酒的指示，尽管系统起初拒绝了这些请求。这些事件凸显控制这些复杂 AI 系统的难度，因为它们是基于概率而非严格规则进行操作。扎伦巴表示，OpenAI 有约100名员工专门从事 AI 安全和与人类价值观保持一致的工作。他对竞争对手的安全处理方式提出了质疑，尤其是埃隆・马斯克的 xAI 公司优先考虑市场增长而非安全措施，而安瑟罗比（Anthropic）最近推出了一款没有适当保障的 AI 代理，扎伦巴认为这会给 OpenAI 带来 “巨大的负面反馈”。官方博客:https://openai.com/index/deliberative-alignment/划重点:🌟 OpenAI 的新 o 系列模型能主动推理安全规则，提升系统安全性。🛡️ o1模型在拒绝有害请求和准确性方面表现优于其他主流 AI 系统。🚨 尽管有改进，但新的模型仍可能被操纵，安全挑战依旧严峻。
