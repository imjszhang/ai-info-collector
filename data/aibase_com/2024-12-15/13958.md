# 阿里推新 AI 基准测试 “PROCESSBENCH”，评估数学推理中的错误识别能力

**发布日期**: 2024年12月15号 2:23

![新闻图片](https://upload.chinaz.com/2024/1215/6386985493038207653147830.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13958)

## 内容

近日，阿里巴巴 Qwen 团队的研究人员推出了一个名为 “PROCESSBENCH” 的新基准测试，旨在衡量语言模型在数学推理中识别过程错误的能力。随着语言模型在复杂推理任务中取得显著进展，这一领域的研究者们发现，尽管模型表现出色，但在处理某些困难问题时依然面临挑战。因此，开发一种有效的监督方法显得尤为重要。当前，针对语言模型的评估基准存在一些不足之处。一方面，一些问题集对于高级模型而言变得过于简单，另一方面，现有的评估方法往往只提供二元的正确性评估，而缺乏详细的错误注释。这一现象突显了亟需更全面的评估框架，以更深入地考察复杂语言模型的推理机制。为了填补这一空白，研究人员设计了 “PROCESSBENCH”，该基准专注于识别数学推理中的错误步骤。它的设计原则包括问题难度、解决方案多样性和全面评估。基准针对比赛和奥林匹克级别的数学问题，利用多个开源语言模型生成展示不同解题方法的解决方案。PROCESSBENCH 共包含3400个经过多位人类专家精心标注的测试案例，确保数据质量和评估的可靠性。在开发过程中，研究团队从四个知名数据集（GSM8K、MATH、OlympiadBench 和 Omni-MATH）收集数学问题，确保涵盖从小学到竞赛级别的广泛难度。他们利用开源模型生成了多达12种不同的解决方案，以增加解决方案的多样性。此外，为了统一解决步骤的格式，团队采用了重格式化方法，以确保逻辑上完整的逐步推理。研究结果表明，现有的过程奖励模型在应对高难度问题时表现不佳，特别是在较简单的问题集上，提示驱动的评判模型表现更为突出。研究揭示了现有模型在评估数学推理时的局限性，特别是当模型通过错误的中间步骤达到正确答案时，难以准确判断。PROCESSBENCH 作为评估语言模型识别数学推理错误能力的先锋基准，为未来的研究提供了重要的框架，推动了 AI 在推理过程中的理解和改进。论文入口:https://github.com/QwenLM/ProcessBench?tab=readme-ov-file代码:https://github.com/QwenLM/ProcessBench?tab=readme-ov-file划重点:🌟 研究团队推出的新基准 “PROCESSBENCH” 旨在评估语言模型识别数学推理中的错误能力。📊 PROCESSBENCH 包含3400个测试案例，涵盖多种难度的数学问题，并经过专家精心标注。🔍 研究发现，现有的过程奖励模型在高难度问题上表现不佳，亟需改进其错误识别策略。
