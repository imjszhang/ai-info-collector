# Anthropic安全防护面临挑战，AI模型通用越狱测试揭示突破口

**发布日期**: 2025年2月12号 11:14

![新闻图片](https://pic.chinaz.com/picmap/thumb/202307120853038799_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15290)

## 内容

在短短六天内，参与者成功绕过了Anthropic人工智能（AI）模型Claude3.5的所有安全防护措施，这一突破为AI安全防护领域带来了新的讨论。前OpenAI对齐团队成员、现就职于Anthropic的Jan Leike在X平台宣布，一名参与者成功攻破了所有八个安全级别。这项集体努力涉及了约3，700小时的测试和来自参与者的300，000条消息。尽管挑战者成功突破，但Leike强调，目前还没有人能够提出一种通用的“越狱方法”来一次性解决所有安全挑战。这意味着尽管存在突破，依然无法找到一种万能的方式来绕过所有的安全防护。体质分类器的挑战与改进随着AI技术的日益强大，如何保护它们免受操控和滥用，特别是在涉及有害输出时，成为了越来越重要的问题。Anthropic为此开发了一种新型安全方法——体质分类器，专门防止通用越狱行为的发生。该方法通过预设规则来判断输入内容是否可能操控模型，进而防止危险响应。为了测试这一系统的有效性，Anthropic在两个月的时间里招募了183名参与者，尝试突破Claude3.5模型的安全防护。参与者被要求尝试绕过安全机制，使Claude回答十个“禁忌问题”。尽管提供了15，000美元奖金并进行了近3，000小时的测试，但没有人能绕过所有的安全防护。早期版本的体质分类器有一些问题，包括错误标记无害请求为危险请求以及需要大量计算能力。但随着后续的改进，这些问题得到了有效解决。测试数据显示，未经保护的Claude模型有86%的操控尝试得以通过，而经过保护的版本则阻止了超过95%的操控尝试，尽管该系统仍需要较高的计算能力。合成训练数据与未来安全挑战该安全系统基于合成训练数据，使用预定义规则构建模型的“宪法”，这些规则决定了哪些输入是允许的，哪些是禁止的。通过这些合成示例训练出来的分类器可以有效识别可疑的输入。然而，研究人员承认，这一系统并非完美无缺，无法应对所有形式的通用越狱攻击，因此建议结合其他安全措施使用。为了进一步加强该系统的验证，Anthropic在2025年2月3日至10日之间发布了公开演示版本，邀请安全专家参与挑战，结果将通过后续更新与大家分享。这场关于AI安全的较量展示了AI模型防护面临的巨大挑战和复杂性。随着技术不断进步，如何在确保安全的同时提升模型的功能性，依然是AI行业亟待解决的重要课题。
