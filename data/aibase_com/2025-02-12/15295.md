# 字节跳动豆包UltraMem架构将大模型推理成本降低83%

**发布日期**: 2025年2月12号 14:04

![新闻图片](https://pic.chinaz.com/picmap/thumb/202405160822226470_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15295)

## 内容

字节跳动豆包大模型团队今日宣布，成功研发出全新稀疏模型架构UltraMem，该架构有效解决了MoE（混合专家）模型推理时的高额访存问题，推理速度较MoE提升2-6倍，推理成本最高可降低83%。这一突破性进展为大模型的高效推理开辟了新路径。UltraMem架构在保证模型效果的前提下，成功解决了MoE架构推理时的访存瓶颈。实验结果表明，在参数和激活条件相同的情况下，UltraMem不仅模型效果优于MoE，更将推理速度提升了2-6倍。此外，在常见batch size规模下，UltraMem的访存成本几乎与同计算量的Dense模型相当，显著降低了推理成本。研究团队训练了规模达2000万value的UltraMem模型，实验结果显示，在同等计算资源下，该模型同时实现了业界领先的推理速度和模型性能。这一成果验证了UltraMem架构的优异Scaling特性，为构建数十亿规模value或expert模型奠定了技术基础。随着大模型规模不断扩大，推理成本和速度成为制约其应用的关键因素。尽管MoE架构已实现计算与参数解耦，但其推理时的高访存需求导致延迟增加。UltraMem架构的提出，有效解决了这一难题，为大模型的规模化应用提供了新的技术选择。
