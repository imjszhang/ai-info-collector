# Mistral更新编码模型Codestral25.01:速度提升，性能领跑行业

**发布日期**: 2025年1月14号 9:55

![新闻图片](https://pic.chinaz.com/picmap/thumb/202310191515152862_5.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14679)

## 内容

Mistral公司近日宣布推出其最新的开源编码模型——Codestral25.01，这是其备受欢迎的编码模型Codestral的升级版。这一版本在架构上进行了优化，显著提升了性能，成为其“重量级编码的明显领导者”，速度比前版提升了两倍。与原版Codestral相似，Codestral25.01依然专注于低延迟和高频率操作，支持代码校正、测试生成以及中间填充任务。Mistral公司表示，这一版本特别适合需要更多数据和模型驻留的企业。基准测试显示，Codestral25.01在Python编码测试中的表现超出预期，HumanEval测试得分为86.6%，远超前版、Codellama70B Instruct以及DeepSeek Coder33B Instruct。开发人员可以通过Mistral IDE插件以及本地部署工具Continue来访问该模型，此外，Mistral还提供了通过Google Vertex AI和Mistral la Plateforme访问API的方式。该模型目前在Azure AI Foundry上提供预览，并将在不久后登陆Amazon Bedrock平台。自去年发布以来，Mistral的Codestral已成为以代码为核心的开源模型中的佼佼者。其首版Codestral是一个22B参数的模型，支持多达80种语言，并在编码性能上优于许多同类产品。紧接着，Mistral推出了Codestral-Mamba，这是一个基于Mamba架构的代码生成模型，能够处理更长的代码串并应对更多的输入需求。Codestral25.01的推出引发了开发者的广泛关注，在发布后的短短几个小时内便在C o pilot Arena排行榜上位居前列。这一趋势表明，专门化的编码模型正在迅速成为开发人员的首选，尤其是在编码任务领域，相较于多功能通用模型，专注型编码模型的需求愈加明显。尽管像OpenAI的o3和Anthropic的Claude这样的通用模型也能进行编码，但专门优化的编码模型在性能上往往更为出色。过去一年内，多个企业发布了针对编码的专用模型，例如阿里巴巴的Qwen2.5-Coder和中国DeepSeek Coder，后者更是成为首个超越GPT-4Turbo的模型。此外，微软也推出了基于专家混合模型（MOE）的GRIN-MoE，该模型不仅能编码，还能解决数学问题。尽管开发者仍在争论选择通用模型还是专注模型，但编码模型的快速崛起显现了对于高效、精准编码工具的巨大需求。凭借专为编码任务训练的优势，Codestral25.01无疑在未来的编码领域占据了一席之地。
