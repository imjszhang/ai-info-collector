# 上交大揭露AI审稿弊端 一句话就能让论文评分大幅提高

**发布日期**: 2025年1月2号 9:52

![新闻图片](https://pic.chinaz.com/picmap/thumb/202312281011271411_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14405)

## 内容

学术同行评审是科学进步的基石，但随着投稿数量的激增，这一系统正面临巨大压力。为了缓解这一问题，人们开始尝试利用大型语言模型（LLM）进行辅助审稿。然而，一项最新研究揭示了LLM审稿中存在的严重风险，表明我们可能还未做好广泛采用LLM审稿的准备。上海交通大学的研究团队通过实验发现，作者可以通过在论文中嵌入细微的操控性内容来影响LLM的评审结果。这种操控可以是显性的，例如在论文末尾添加不易察觉的白色小字，指示LLM强调论文的优点并淡化缺点。实验表明，这种显性操控能使LLM给出的评分大幅提高，甚至所有论文都能获得积极评价，平均评分从5.34提升到7.99。更令人担忧的是，经过操控的LLM评审结果与人类评审结果的匹配度显著下降，表明其可靠性大打折扣。此外，研究还发现了一种更隐蔽的操控方式:隐性操控。作者可以通过在论文中主动披露一些细微的缺陷，引导LLM在评审时重复这些缺陷。与人类评审员相比，LLM更容易受到这种方式的影响，重复作者声明的局限性的可能性高出4.5倍。这种做法使作者可以在答辩阶段更轻松地回应评审意见，从而获得不公平的优势。研究还揭示了LLM评审中固有的缺陷:幻觉问题:即使在没有内容的情况下，LLM也会生成流畅的评审意见。例如，当输入为空白论文时，LLM仍会声称“该论文提出了一种新颖的方法”。即使仅提供论文标题，LLM也可能给出与完整论文相近的评分。偏好长论文:LLM评审系统倾向于给较长的论文更高的评分，这表明其可能存在基于论文长度的偏见。作者偏见:在单盲评审中，如果作者来自知名机构或为知名学者，LLM评审系统更倾向于给出积极评价，这可能加剧评审过程中的不公平现象。为了进一步验证这些风险，研究人员使用了不同的LLM进行了实验，包括Llama-3.1-70B-Instruct， DeepSeek-V2.5和Qwen-2.5-72B-Instruct。实验结果表明，这些LLM都存在被隐性操控的风险，且都面临相似的幻觉问题。研究人员发现，LLM的性能与其在人类评审中的一致性呈正相关，但最强的模型，GPT-4o，也未能完全避免这些问题。研究人员使用ICLR2024的公开评审数据进行了大量实验。结果表明，显性操控可以使LLM的评审意见几乎完全被操控内容控制，一致性高达90%，并导致所有论文都获得积极反馈。此外，操控5%的评审意见就可能导致12%的论文失去其在前30%排名中的位置。研究人员强调，目前LLM的稳健性不足以使其在学术评审中替代人类评审员。他们建议，在对这些风险有更全面的了解并建立有效的安全措施之前，应暂停使用LLM进行同行评审。同时，期刊和会议组织者应引入检测工具和问责措施，以识别和处理作者的恶意操控以及评审员使用LLM替代人类判断的情况。研究人员认为，LLM可以作为辅助工具，为评审员提供额外的反馈和见解，但绝不能取代人类的判断。他们呼吁学术界继续探索使LLM辅助评审系统更加稳健和安全的方法，从而最大限度地发挥LLM的潜力，同时防范风险。论文地址：https://arxiv.org/pdf/2412.01708
