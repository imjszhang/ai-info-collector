# 字节和中国科大出品！VMix：提升扩散模型美学的扩展，即插即用

**发布日期**: 2025年1月2号 10:00

![新闻图片](https://upload.chinaz.com/2025/0102/6387140878779666232371968.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14406)

## 内容

在文本生成图像的领域，扩散模型展现出了非凡的能力，但在美学图像生成方面仍存在一定的不足。最近，来自字节跳动和中国科学技术大学的研究团队提出了一种名为 “Cross-Attention Value Mixing Control”（VMix）适配器的新技术，旨在提升生成图像的质量，并保持对各种视觉概念的通用性。VMix 适配器的核心思想在于，通过设计优越的条件控制方法，增强现有扩散模型的美学表现，同时确保图像与文本之间的对齐。该适配器主要通过两个步骤实现其目标:首先，它通过初始化美学嵌入，将输入的文本提示分解为内容描述和美学描述;其次，在去噪过程的过程中，通过混合交叉注意力的方式，将美学条件融入其中，，从而提升图片的美学效果，且保持图片和提示词的一致性。。这种方法的灵活性使得 VMix 能够在不进行重训练的情况下，应用于多个社区模型，从而提高视觉表现。研究人员通过一系列实验验证了 VMix 的有效性，结果显示该方法在美学图像生成方面的表现超越了其他最先进的方法。同时，VMix 还与多种社区模块（如 LoRA、ControlNet 和 IPAdapter）兼容，进一步拓宽了其应用范围。VMix 的美学细粒度控制能力，体现在调整美学嵌入时，可以通过单维美学标签来改善图像的特定维度，或者通过完整的正面美学标签来整体提升图像质量。在实验中，当用户给定如 “一个女孩靠在窗边，微风拂过，夏日肖像，半身中景” 的文本描述时，VMix 适配器能够显著提升生成图像的美感。VMix 适配器为提升文本到图像生成的美学质量开辟了新的方向，未来有望在更广泛的应用中发挥其潜力。项目入口:https://vmix-diffusion.github.io/VMix/划重点:🌟 VMix 适配器通过美学嵌入将文本提示分解为内容和美学描述，增强图像生成质量。🖼️ 该适配器兼容多个社区模型，用户无需重训练即可提升图像视觉效果。✨ 实验结果表明，VMix 在美学生成方面的表现优于现有技术，具有广泛的应用潜力。
