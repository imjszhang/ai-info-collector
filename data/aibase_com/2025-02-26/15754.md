# 通义新一代轻量化大语言模型DistilQwen2.5 发布 提升性能与效率

**发布日期**: 2025年2月26号 16:58

![新闻图片](https://pic.chinaz.com/picmap/thumb/202305091556144476_5.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15754)

## 内容

在大语言模型逐渐普及的今天，如何在资源受限的环境中实现高效部署已成为重要课题。为了解决这一挑战，基于 Qwen2.5的轻量化大模型系列 DistilQwen2.5正式发布。该模型采用了创新的双层蒸馏框架，通过优化数据和参数融合技术，不仅保留了模型的性能，同时显著降低了计算资源的消耗。DistilQwen2.5的成功得益于其独特的知识蒸馏技术。这一过程首先需要大量高质量的指令数据，这些数据来自多个开源数据集及私有合成数据集。为确保数据的多样性，研究团队通过 Qwen-max 扩展了中英文数据，从而实现了任务和语言的均衡。此后，模型通过采用 “黑盒化蒸馏” 的方式，利用教师模型的输出进行指令的扩展、选择与改写。这种方法不仅提升了数据的质量，还增强了模型的多任务处理能力。值得注意的是，DistilQwen2.5还引入了白盒化蒸馏技术，通过模仿教师模型的 its 分布，使得学生模型在知识获取上更为高效。这种技术避免了传统白盒化蒸馏面临的 GPU 内存消耗、存储与读取速度慢等问题。经过多个权威指令遵循评测基准的测试，DistilQwen2.5的表现令人瞩目，尤其是在 AlpacaEval2.0和 MT-Bench 的评测中表现优异。这标志着轻量化大语言模型的发展进入了一个新的阶段，能够在保证性能的前提下，大幅降低计算成本，进一步推动了 AI 技术在各种应用场景中的落地。DistilQwen2.5的开源发布也将为更多开发者提供便利，使他们能更轻松地使用这一强大的工具，为人工智能技术的普及贡献力量。
