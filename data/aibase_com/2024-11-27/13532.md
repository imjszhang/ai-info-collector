# "佛系"优化器C-AdamW：一行代码，让大模型训练速度狂飙1.47倍！

**发布日期**: 2024年11月27号 16:51

![新闻图片](https://pic.chinaz.com/picmap/thumb/202310191515189884_11.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13532)

## 内容

在 AI 的世界里，"大力出奇迹" 似乎成了金科玉律。模型越大，数据越多，算力越强，仿佛就能越接近智能的圣杯。然而，这狂飙突进的背后，也隐藏着巨大的成本和能耗压力。为了让 AI 训练更高效，科学家们一直在寻找更强大的优化器，就像一位教练，引导模型的参数不断优化，最终达到最佳状态。AdamW 作为 Transformer 预训练的默认优化器，多年来一直是业界标杆。然而，面对日益庞大的模型规模，AdamW 也开始显得力不从心。难道就没有一种方法，既能提升训练速度，又能降低能耗吗?别急，一个全华人团队带着他们的 "秘密武器" C-AdamW 来啦!C-AdamW 全称 Cautious AdamW，中文名 "谨慎 AdamW"，是不是听起来就很 "佛系"?没错，C-AdamW 的核心思想就是 "三思而后行"。想象一下，模型的参数就像一群精力旺盛的小朋友，总想四处乱跑。AdamW 就像一位尽职尽责的老师，努力引导他们朝着正确的方向前进。但有时候，小朋友们会过于兴奋，跑错了方向，反而浪费了时间和精力。这时候，C-AdamW 就像一位智慧的长者，戴着一副 "火眼金睛"，能够精准识别更新方向是否正确。如果方向错了，C-AdamW 就会果断喊停，避免模型在错误的道路上越走越远。这种 "谨慎" 的策略，保证了每次更新都能有效地降低损失函数，从而加快模型的收敛速度。实验结果表明，C-AdamW 在 Llama 和 MAE 预训练中，将训练速度提升至1.47倍!更重要的是，C-AdamW 几乎没有额外的计算开销，只需对现有代码进行一行简单的修改即可实现。这意味着，开发者们可以轻松地将 C-AdamW 应用到各种模型训练中，享受 "速度与激情"!C-AdamW 的 "佛系" 之处，还在于它保留了 Adam 的哈密顿函数，并在李雅普诺夫分析下不破坏收敛性保证。这意味着，C-AdamW 不仅速度更快，而且稳定性也得到了保障，不会出现训练崩溃等问题。当然，"佛系" 不代表 "不思进取"。研究团队表示，他们将继续探索更丰富的 ϕ 函数，并在特征空间而非参数空间中应用掩码，以进一步提升 C-AdamW 的性能。可以预见，C-AdamW 将成为深度学习领域的新宠，为大模型训练带来革命性的改变!论文地址:https://arxiv.org/abs/2411.16085GitHub:https://github.com/kyleliang919/C-Optim
