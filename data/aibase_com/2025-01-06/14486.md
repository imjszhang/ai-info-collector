# ​NVIDIA联合高校发布 “FlashInfer”：提升大语言模型推理效率的全新内核库

**发布日期**: 2025年1月6号 10:36

![新闻图片](https://upload.chinaz.com/2025/0106/6387175654164175147285788.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14486)

## 内容

随着大语言模型（LLM）在现代人工智能应用中的广泛应用，诸如聊天机器人和代码生成器等工具依赖于这些模型的能力。然而，随之而来的推理过程中的效率问题也日益突出。尤其是在处理注意力机制时，如 FlashAttention 和 SparseAttention，面对多样化的工作负载、动态输入模式以及 GPU 资源限制时，往往显得力不从心。这些挑战加上高延迟和内存瓶颈，迫切需要更高效、灵活的解决方案，以支持可扩展和响应迅速的 LLM 推理。为了解决这一问题，来自华盛顿大学、NVIDIA、Perplexity AI 和卡内基梅隆大学的研究人员共同开发了 FlashInfer，这是一个专门为 LLM 推理设计的人工智能库和内核生成器。FlashInfer 提供了高性能的 GPU 内核实现，涵盖多种注意力机制，包括 FlashAttention、SparseAttention、PageAttention 及采样。其设计理念强调灵活性和效率，旨在应对 LLM 推理服务中的关键挑战。FlashInfer 的技术特点包括:1. *全面的注意力内核 :支持多种注意力机制，包括预填充、解码和追加注意力，兼容各种 KV-cache 格式，提升单请求和批量服务场景的性能。2. *优化的共享前缀解码 :通过分组查询注意力（GQA）和融合的旋转位置嵌入(RoPE)注意力，FlashInfer 实现了显著的速度提升，例如在长提示解码方面，比 vLLM 的 Page Attention 实现快31倍。3.  动态负载平衡调度 :FlashInfer 的调度器能根据输入变化动态调整，减少 GPU 空闲时间，确保高效利用。它与 CUDA Graphs 的兼容性进一步提升了在生产环境中的适用性。在性能方面，FlashInfer 在多个基准测试中表现出色，显著减少了延迟，特别是在处理长上下文推理和并行生成任务中表现出色。在 NVIDIA H100GPU 上，FlashInfer 在并行生成任务中实现了13-17% 的速度提升。其动态调度器和优化的内核显著改善了带宽和 FLOP 利用率，特别是在序列长度不均或均匀的情况下。FlashInfer 为 LLM 推理挑战提供了切实可行且高效的解决方案，大幅提升了性能和资源利用效率。其灵活的设计和集成能力，使其成为推动 LLM 服务框架发展的重要工具。作为一个开源项目，FlashInfer 鼓励研究界的进一步合作与创新，确保在人工智能基础设施领域的持续改进和适应新兴挑战。项目入口：https://github.com/flashinfer-ai/flashinfer划重点:🌟 FlashInfer 是一个新发布的人工智能库，专为大语言模型推理设计，能显著提升效率。⚡ 该库支持多种注意力机制，优化了 GPU 资源利用，减少了推理延迟。🚀 FlashInfer 作为开源项目，欢迎研究者共同参与，推动 AI 基础设施的创新与发展。
