# 字节开源嘴型同步模型LatentSync，实现超真实口型同步

**发布日期**: 2025年1月6号 9:38

![新闻图片](https://upload.chinaz.com/2025/0106/6387175300939244251337832.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14478)

## 内容

近日，字节跳动发布了名为 LatentSync 的新型口型同步框架，旨在利用音频条件潜在扩散模型实现更精确的口型同步。该框架基于Stable Diffusion，针对时间一致性做了优化。与以往的基于像素空间扩散或两阶段生成的方法不同，LatentSync 采用端到端的方式，无需中间运动表示，能够直接建模复杂的音频与视觉之间的关系。在 LatentSync 的框架中，首先使用 Whisper 将音频频谱图转换为音频嵌入，并通过交叉注意力层将其集成到 U-Net 模型中。框架通过将参考帧和掩码帧与噪声潜在变量进行通道级拼接，作为 U-Net 的输入。在训练过程中，采用一步法从预测噪声中估计出干净的潜在变量，然后进行解码以生成干净的帧。同时，模型引入了 Temporal REPresentation Alignment（TREPA）机制，以增强时间一致性，确保生成的视频在口型同步准确性的同时，能够在时间上保持连贯。为了展示该技术的效果，项目提供了一系列示例视频，分别展示了原始视频与经过口型同步处理后的视频。通过示例，用户可以直观地感受到 LatentSync 在视频口型同步方面的显著进步。原始视频：输出视频：此外，项目还计划开源推理代码和检查点，方便用户进行训练和测试。对于想要尝试推理的用户，只需下载必要的模型权重文件，即可进行操作。完整的数据处理流程也已设计好，涵盖了从视频文件处理到面部对齐的各个步骤，确保用户能够轻松上手。模型项目入口：https://github.com/bytedance/LatentSync划重点：🌟 LatentSync 是一个基于音频条件潜在扩散模型的端到端口型同步框架，无需中间运动表示。🎤 该框架利用 Whisper 将音频频谱图转换为嵌入，增强了模型在口型同步过程中的准确性和时间一致性。📹 项目提供了一系列示例视频，并计划开源相关代码和数据处理流程，方便用户使用和训练。
