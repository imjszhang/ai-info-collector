# 黑客利用 “损坏” pickle 文件在 HuggingFace 上传恶意 AI 模型

**发布日期**: 2025年2月10号 11:11

![新闻图片](https://upload.chinaz.com/2025/0210/6387478268805953733878390.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15201)

## 内容

近日，网络安全研究人员发现，在知名机器学习平台 HuggingFace 上，有两个恶意的机器学习模型悄然上传。这些模型使用了一种新奇的技术，通过 “损坏” 的 pickle 文件成功规避了安全检测，令人担忧。ReversingLabs 的研究员卡洛・赞基（Karlo Zanki）指出，从这些 PyTorch 格式的存档中提取的 pickle 文件开头，暗示了其中包含恶意的 Python 代码。这些恶意代码主要是反向 shell，能够连接到硬编码的 IP 地址，实现黑客的远程控制。这种利用 pickle 文件的攻击方法被称为 nullifAI，目的是绕过现有的安全防护措施。具体来说，Hugging Face 上发现的两个恶意模型分别为 glockr1/ballr7和 who-r-u0000/0000000000000000000000000000000000000。这些模型更像是一个概念验证，而不是实际的供应链攻击案例。虽然 pickle 格式在机器学习模型的分发中非常常见，但它也存在着安全隐患，因为该格式允许在加载和反序列化时执行任意代码。研究人员发现，这两个模型使用的是 PyTorch 格式的压缩 pickle 文件，且采用了不同于默认 ZIP 格式的7z 压缩方式。这一特征使得它们能够避开 Hugging Face 的 Picklescan 工具的恶意检测。赞基进一步指出，虽然 pickle 文件中的反序列化会因为恶意载荷的插入而出现错误，但它仍能部分反序列化，从而执行恶意代码。更为复杂的是，由于这些恶意代码位于 pickle 流的开头，Hugging Face 的安全扫描工具未能识别出模型的潜在风险。这一事件引发了对机器学习模型安全性的广泛关注。针对该问题，研究人员已进行修复，并更新了 Picklescan 工具，以防止类似事件的再次发生。此次事件再次提醒了技术界，网络安全问题依旧不容忽视，特别是在 AI 和机器学习的快速发展背景下，保护用户和平台的安全显得尤为重要。划重点:🛡️ 恶意模型使用 “损坏” 的 pickle 文件技术，成功规避了安全检测。🔍 研究人员发现这些模型暗含反向 shell，连接到硬编码的 IP 地址。🔧 Hugging Face 已对安全扫描工具进行更新，修复了相关漏洞。
