# 豆包：视频生成模型 “VideoWorld” 已开源 实现纯视觉学习

**发布日期**: 2025年2月10号 14:22

![新闻图片](https://upload.chinaz.com/2025/0210/6387479415364694857925753.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15207)

## 内容

据豆包大模型团队官方公众号消息，在北京交通大学和中国科学技术大学的联合研究下，由豆包大模型团队提出的 “VideoWorld” 视频生成实验模型近日正式开源。这个模型的最大亮点在于，它不再依赖传统的语言模型，而是仅凭视觉信息就能认知和理解世界。这一突破性的研究灵感来源于李飞飞教授在 TED 演讲中提到的 “幼儿可以不依靠语言理解真实世界” 的理念。“VideoWorld” 通过分析和处理大量视频数据，实现了复杂的推理、规划和决策能力。研究团队的实验显示，模型在仅有300M 参数的情况下，便取得了显著的效果。与现有依赖语言或标签数据的模型不同，VideoWorld 能够独立进行知识学习，尤其在折纸、打领结等复杂任务中，能够提供更加直观的学习方式。为了验证该模型的有效性，研究团队搭建了围棋对战和机器人模拟操控两种实验环境。围棋作为一项高度策略性游戏，可以有效评估模型的规则学习和推理能力，而机器人任务则考察模型在控制和规划方面的表现。在训练阶段，模型通过观看大量视频演示数据，逐步建立起对未来画面的预测能力。为了提高视频学习的效率，团队引入了一种潜在动态模型（LDM），旨在压缩视频帧之间的视觉变化，从而提取出关键信息。这一方法不仅减少了冗余信息，还增强了模型对复杂知识的学习效率。通过这一创新，VideoWorld 在围棋和机器人任务中展示了出色的能力，甚至达到了专业五段围棋的水平。论文链接:https://arxiv.org/abs/2501.09781代码链接:https://github.com/bytedance/VideoWorld项目主页:https://maverickren.github.io/VideoWorld.github.io划重点:🌟 “VideoWorld” 模型仅凭视觉信息即可实现知识学习，不依赖语言模型。🤖 模型在围棋和机器人模拟任务中展现出卓越的推理和规划能力。🔓 该项目代码与模型已开源，欢迎各界人士参与体验与交流。
