# ​无须超级集群！Nous Research 启动全球分布式 AI 训练，颠覆大模型开发方式

**发布日期**: 2024年12月3号 10:08

![新闻图片](https://upload.chinaz.com/2024/1203/6386881727291963739676302.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13640)

## 内容

在快速发展的生成式 AI 领域，Nous Research 团队正在进行一项独特的实验:他们正在利用分布在全球的机器，预训练一款15亿参数的大型语言模型（LLM），这一过程避免了传统上需要在昂贵且耗电的数据中心或超级集群中进行的集中式开发。Nous Research 还在其专门网站 distro.nousresearch.com 上直播这一预训练过程，实时展示模型在各类评估基准上的表现，并提供参与训练的硬件位置地图，涵盖美国和欧洲的多个地点。截至本文发布时，预训练的剩余时间约为57小时（即2.3天），而已完成的训练进度超过75%。预训练是训练 LLM 的第一步，也是最基础的步骤，它涉及对大量文本数据的训练，以学习语言的统计特性和结构。在这一阶段，模型通过处理广泛的文本数据集，捕捉语言的模式、语法和词汇间的上下文关系。这一过程使模型具备了对语言的广泛理解，能够生成连贯的文本并执行多种语言相关任务。在预训练之后，模型还需进行针对特定任务或领域的微调。如果这一计划成功，Nous Research 将证明在没有昂贵超级集群或低延迟传输的情况下，依然可以训练出前沿级别的 LLM，标志着分布式 AI 训练的新纪元。这种开放源代码的训练方法可能会改变生成式 AI 的力量格局，使小型团队和非企业行为者在这一领域具备更多竞争力。Nous 使用的这一新技术名为 Nous DisTrO（Distributed Training Over-the-Internet），旨在减少预训练过程中 GPU 间的通信带宽需求。根据 Nous Research 的最新发布，DisTrO 能够将通信需求降低多达10，000倍，使得在较慢且经济实惠的互联网连接条件下，依然能保持竞争力的收敛率和损失曲线。此外，DisTrO 的核心突破在于有效压缩 GPU 间交换的数据量，而不影响模型的性能。这一技术建立在早期的去耦动量优化算法（DeMo）基础之上，后者同样旨在大幅减少 GPU 间的通信需求，同时保持训练性能。硬件方面，Nous Research 的预训练过程得到了 Oracle、Lambda Labs、Northern Data Group、Crusoe Cloud 和 Andromeda Cluster 等多家知名合作伙伴的支持，共同提供所需的异构硬件，充分测试 DisTrO 在实际分布式环境下的能力。博客入口:https://nousresearch.com/划重点:🌐 Nous Research 正在进行全球分布式 AI 训练，旨在预训练一款15亿参数的大型语言模型。💻 使用 Nous DisTrO 技术，该过程显著降低了 GPU 间的通信带宽需求，使得低成本训练成为可能。🤝 该项目得到了多家硬件供应商的支持，推动了分布式 AI 研究的进展。
