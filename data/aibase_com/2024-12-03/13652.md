# ​Liquid AI 推出 STAR 模型架构，效率超越传统 Transformer

**发布日期**: 2024年12月3号 14:29

![新闻图片](https://upload.chinaz.com/2024/1203/6386883293099900247735903.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13652)

## 内容

在当前大型语言模型（LLM）开发的竞争中，各大人工智能公司面临着越来越多的挑战，因此，越来越多的目光开始转向 “Transformer” 以外的替代架构。Transformer 架构自2017年由谷歌研究人员提出以来，已成为当今生成式人工智能的基础。为了应对这一挑战，由麻省理工学院孵化的初创公司 Liquid AI 推出了一种名为 STAR(Synthesis of Tailored Architectures)的创新框架。STAR 框架利用进化算法和数值编码系统，旨在自动化生成和优化人工智能模型架构。Liquid AI 的研究团队指出，STAR 的设计方法不同于传统架构设计，它采用了分层编码技术，称为 “STAR 基因组”，从而探索潜在架构的广泛设计空间。通过基因组的组合与变异，STAR 能够合成和优化符合特定性能和硬件需求的架构。在针对自回归语言建模的测试中，STAR 显示出优于传统优化 Transformer++ 和混合模型的能力。在优化质量和缓存大小方面，STAR 进化的架构相较于混合模型的缓存大小减少了高达37%，而相较于传统 Transformer 则达到了90% 的减少。这种高效性并未牺牲模型的预测性能，反而在某些情况下超越了竞争对手。研究还表明，STAR 的架构可扩展性强，一个从1.25亿参数扩展到10亿参数的 STAR 进化模型在标准基准测试中表现与现有的 Transformer++ 和混合模型相当或更好，同时显著降低了推理缓存需求。Liquid AI 表示，STAR 的设计理念融入了动态系统、信号处理和数值线性代数的原理，构建了一个灵活的计算单元搜索空间。STAR 的一大特色在于其模块化设计，使得它能够在多个层次上编码和优化架构，为研究人员提供了洞察有效架构组件组合的机会。Liquid AI 认为 STAR 的高效架构合成能力将应用于各种领域，尤其是在需要平衡质量与计算效率的场景。虽然 Liquid AI 尚未公布具体的商业部署或定价计划，但其研究成果标志着自动化架构设计领域的一次重大进步。随着 AI 领域的不断演进，像 STAR 这样的框架可能会在塑造下一代智能系统中发挥重要作用。官方博客:https://www.liquid.ai/research/automated-architecture-synthesis-via-targeted-evolution划重点:🌟 Liquid AI 推出的 STAR 框架通过进化算法自动生成和优化 AI 模型架构。📉 STAR 模型在缓存大小方面减少了高达90%，并在性能上超过传统 Transformer。🔍 STAR 的模块化设计可应用于多个领域，推动 AI 系统优化的进一步发展。
