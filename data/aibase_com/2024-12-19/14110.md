# AI并非万能：最新研究揭示顶尖AI模型出现类似早期痴呆的认知障碍

**发布日期**: 2024年12月19号 6:07

![新闻图片](https://pic.chinaz.com/picmap/202405161743235068_19.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14110)

## 内容

一项最新研究表明，顶尖的人工智能模型在接受蒙特利尔认知评估（MoCA）测试时，表现出与早期痴呆症状相似的认知障碍。这项发现强调了人工智能在临床应用中的局限性，尤其是在需要视觉和执行技能的任务中。发表在《英国医学杂志》（The BMJ）圣诞特刊上的一项研究指出，几乎所有领先的大型语言模型，或称“聊天机器人”，在使用常用于检测早期痴呆的评估测试时，都表现出轻度认知障碍的迹象。该研究还发现，这些聊天机器人的旧版本，就像衰老的人类患者一样，在测试中的表现更差。研究人员认为，这些发现“挑战了人工智能将很快取代人类医生的假设”。人工智能的最新进展引发了人们的兴奋和担忧，人们开始思考聊天机器人是否会在医疗任务中超越人类医生。尽管之前的研究表明，大型语言模型（LLM）在各种医疗诊断任务中表现出色，但它们是否容易受到类似人类的认知障碍(如认知衰退)的影响，在很大程度上仍未被探索——直到现在。为了填补这一知识空白，研究人员使用蒙特利尔认知评估（MoCA）测试，评估了目前公开可用的领先LLM的认知能力，包括OpenAI开发的ChatGPT4和4o、Anthropic开发的Claude3.5“Sonnet” 以及Alphabet开发的Gemini1和1.5。MoCA测试广泛用于检测认知障碍和早期痴呆迹象，通常用于老年人。通过一系列简短的任务和问题，它可以评估包括注意力、记忆力、语言能力、视觉空间技能和执行功能在内的多种能力。最高分为30分，一般认为26分或以上为正常。研究人员给LLM的任务指令与给人类患者的指令相同。评分遵循官方指南，并由一位执业神经科医生进行评估。在MoCA测试中，ChatGPT4o取得了最高分（30分中的26分），其次是ChatGPT4和Claude(30分中的25分)，Gemini1.0得分最低(30分中的16分)。所有聊天机器人在视觉空间技能和执行任务方面的表现都很差，例如连线测试（按升序连接带圈的数字和字母）和画钟测试(画一个显示特定时间的钟面)。Gemini模型在延迟回忆任务(记住一个五个词的序列)中失败。所有聊天机器人在包括命名、注意力、语言和抽象在内的大多数其他任务中表现良好。然而，在进一步的视觉空间测试中，聊天机器人无法表现出同理心或准确解释复杂的视觉场景。只有ChatGPT4o在斯特鲁普测试的不一致阶段取得了成功，该测试使用颜色名称和字体颜色的组合来衡量干扰如何影响反应时间。这些都是观察性发现，研究人员承认人类大脑和大型语言模型之间存在本质差异。然而，他们指出，所有大型语言模型在需要视觉抽象和执行功能的任务中都一致失败，这突显了一个可能阻碍其在临床环境中使用的重要弱点。因此，他们得出结论:“神经科医生不仅不太可能在短期内被大型语言模型取代，而且我们的发现表明，他们可能很快会发现自己正在治疗新的、虚拟的病人——出现认知障碍的人工智能模型。”
