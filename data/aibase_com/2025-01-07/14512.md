# 谷歌利用小模型加速 AI 训练大模型，提升 28% 效率

**发布日期**: 2025年1月7号 10:17

![新闻图片](https://pic.chinaz.com/picmap/202209071519247086_3.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14512)

## 内容

近年来，训练大型语言模型（LLM）变得越来越昂贵且复杂，只有少数大型科技公司具备相应的计算资源。不过，谷歌最近推出了一种名为 SALT(小模型辅助大模型训练)的新方法，这一创新可能会彻底改变 AI 训练的格局。图源备注:图片由AI生成，图片授权服务商Midjourney根据谷歌研究和 DeepMind 的最新研究论文，“一点帮助就能走得更远:通过利用小型语言模型实现高效的 LLM 训练”，SALT 引入了一种新的两阶段训练过程。这种方法不仅高效，而且更具实用性，改变了我们以往的训练方式。SALT 的第一阶段是知识蒸馏。在这一阶段中，小型语言模型（SLM）充当教师，将其理解知识传递给更大的模型。小型模型通过 “软标签” 来分享其学习到的知识，帮助大型模型在学习的初期掌握基础概念。这个阶段尤其适用于小型模型在学习区域具有较强预测信心的 “简单” 任务。第二阶段是自我监督学习。大型模型在这一阶段开始独立学习，专注于掌握更复杂的模式和挑战性任务。这个转变需要经过精心设计的策略，包括线性衰减和线性比例衰减，这确保了大型模型能够平稳过渡，逐步减少对小型模型的依赖。谷歌研究人员在实验中发现，使用一个15亿参数的小型模型训练一个28亿参数的大型模型，在 “堆栈数据集” 上的训练时间缩短了28%。在经过微调后，大型模型在数学问题的准确率从31.84% 提升至34.87%，阅读理解的准确率也从63.7% 提高到67%。这种新方法不仅提升了训练效率，还在性能上取得了显著进步。SALT 的出现有望降低 AI 开发的门槛，使得许多原本受限于资源的小型研究机构和公司也能参与到 AI 模型的开发中。研究和开发的机会将更加普及，可能会催生出更多独特和专业化的 AI 解决方案，带动相关领域的创新与应用。划重点:🌟 采用 SALT 方法可以将大型模型的训练时间缩短28%，极大降低了计算成本。📈 使用小型模型进行知识蒸馏，能够显著提升大型模型在复杂任务上的表现。🔍 SALT 的创新可能会降低 AI 开发的门槛，使得更多小型机构能够参与到 AI 研究中。
