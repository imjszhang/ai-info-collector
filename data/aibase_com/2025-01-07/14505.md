# GPT-4o 级别！VITA-1.5：实时视觉与语音交互， 1.5秒互动延迟

**发布日期**: 2025年1月7号 9:30

![新闻图片](https://upload.chinaz.com/2025/0107/6387183881877654506927262.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14505)

## 内容

近日，VITA-MLLM 团队宣布推出 VITA-1.5，这是该团队在 VITA-1.0基础上推出的升级版本，致力于提升多模态交互的实时性与准确性。VITA-1.5不仅支持英语和中文，还在多项性能指标上实现了显著提升，为用户提供了更流畅的交互体验。在 VITA-1.5中，互动延迟大幅降低，从原来的4秒缩短至仅1.5秒，用户在进行语音交互时几乎感受不到延迟。此外，该版本在多模态性能方面也有显著提高，经过评估，VITA-1.5在 MME、MMBench 和 MathVista 等多个基准测试中的平均性能从59.8提升至70.8，展现了出色的能力。VITA-1.5在语音处理能力上也进行了深度优化。其自动语音识别（ASR）系统的错误率显著降低，从18.4降至7.5，这使得语音指令的理解和响应更加准确。同时，VITA-1.5引入了一个端到端的文本转语音(TTS)模块，该模块能够直接接受大型语言模型(LLM)的嵌入作为输入，从而提高语音合成的自然度和连贯性。为了确保多模态能力的平衡，VITA-1.5采用了渐进式训练策略，使得新增的语音处理模块对视觉 - 语言的表现影响最小，图像理解性能从71.3轻微下降至70.8。团队通过这些技术创新，进一步推动了实时视觉与语音交互的界限，为未来的智能交互应用奠定了基础。在 VITA-1.5的使用方面，开发者可以通过简单的命令行操作进行快速入门，并且提供了基础和实时互动演示。用户需要准备一些必要的模块，例如语音活动检测（VAD）模块，以提升实时交互体验。此外，VITA-1.5还将开源其代码，方便广大开发者参与和贡献。VITA-1.5的推出标志着交互式多模态大语言模型领域的又一重要进展，展现了该团队在技术创新和用户体验上的不懈追求。项目入口:https://github.com/VITA-MLLM/VITA?tab=readme-ov-file划重点:🌟 VITA-1.5大幅降低互动延迟，从4秒缩短至1.5秒，显著提升用户体验。📈 多模态性能提升，多个基准测试的平均性能从59.8提升至70.8。🔊 语音处理能力增强，ASR 错误率从18.4降至7.5，语音识别更准确。
