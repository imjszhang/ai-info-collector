# GPT做不好图生成？G2PT 模型利用序列化技术提升效率与质量

**发布日期**: 2025年1月7号 17:15

![新闻图片](https://pic.chinaz.com/picmap/202403150934201770_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14538)

## 内容

在科学与技术的交汇点上，图（Graphs）作为表达复杂关系的重要工具，正在逐渐受到研究者的关注。从化学分子设计到社交网络分析，图在众多领域扮演着不可或缺的角色。然而，如何高效、灵活地生成图形，一直以来都是一个颇具挑战性的问题。近日，塔夫茨大学、东北大学和康奈尔大学的研究团队联手推出了一项名为 Graph Generative Pre-trained Transformer(G2PT)的自回归模型，旨在重新定义图生成与表示方式。图源备注：图片由AI生成，图片授权服务商Midjourney与传统图生成模型依赖邻接矩阵（adjacency matrix）不同，G2PT 引入了一种基于序列的 tokenization 方法。这种方法通过将图分解为节点集和边集，充分利用了图的稀疏性，从而显著提高了计算效率。G2PT 的创新之处在于能够像处理自然语言一样，逐步生成图，并通过预测下一个 token 的方式完成整个图的构建。研究表明，这种序列化的表示方式不仅减少了 token 的数量，还提升了生成质量。G2PT 的适应性和扩展性令人瞩目。通过 Fine-tuning 技术，它在目标导向图生成和图属性预测等任务中展现了卓越的性能。例如，在药物设计中，G2PT 能够生成具有特定理化性质的分子图。此外，通过提取预训练模型的图嵌入，G2PT 在多个分子属性预测数据集上也表现出了优越性。在对比实验中，G2PT 在多个基准数据集上的表现均显著优于现有的最先进模型。在生成有效性、唯一性和分子属性分布匹配等方面，它的表现都得到了高度认可。研究人员还分析了模型和数据规模对生成性能的影响，结果显示，随着模型规模的增加，生成性能显著提升，且在一定规模后趋于饱和。尽管 G2PT 在多个任务中展现出了卓越的能力，但研究者们也指出，生成顺序的敏感性可能意味着不同图域需要不同的顺序优化策略。未来的研究有望进一步探索更通用且表达力更强的序列设计。G2PT 的出现，不仅为图生成领域带来了创新的方法，也为相关领域的研究与应用奠定了坚实的基础。
