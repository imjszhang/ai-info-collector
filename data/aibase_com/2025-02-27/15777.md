# DeepSeek开源第四天发布并行策略升级:DualPipe与EPLB技术推动大模型训练革命

**发布日期**: 2025年2月27号 10:45

![新闻图片](https://pic.chinaz.com/picmap/thumb/202502051558233072_8.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15777)

## 内容

今日，国内人工智能领军企业DeepSeek正式公开其开源计划的第四日成果——Optimized Parallelism Strategies（优化并行策略），重点推出双向管道并行算法DualPipe、专家并行负载均衡器EPLB，以及对计算-通信重叠机制的深度优化。此次技术升级直击大规模语言模型训练的核心痛点，为超万卡级集群的高效运行提供了全新解决方案。1. DualPipe:双向管道并行算法作为本次升级的核心技术之一，DualPipe专为V3/R1架构设计，通过创新的双向数据流管道，实现计算与通信的高度重叠。相较于传统单向流水线，该技术可显著提升计算吞吐量，尤其适用于千亿至万亿参数规模的模型训练。GitHub代码库显示，DualPipe通过智能调度机制，在反向传播阶段同步执行前向计算，使硬件利用率提升约30%。项目链接:https://github.com/deepseek-ai/DualPipe2. EPLB:动态负载均衡器针对混合专家（MoE）模型训练中的“热点专家”顽疾，EPLB技术首次实现专家并行的动态负载平衡。传统方法因专家任务分配不均常导致部分计算卡过载，而EPLB通过实时监控与自适应分配，使万卡级集群的整体利用率提升至92%以上，有效避免资源闲置。项目链接:https://github.com/deepseek-ai/EPLB3. 计算-通信重叠优化基于V3/R1架构的通信重叠分析工具，DeepSeek首次构建了3D并行（数据/流水线/张量并行）的时空效率模型。通过开源的分析数据集开发者可精准定位计算与通信的冲突节点，为超大规模模型训练提供调优基准，据测试可减少约15%的端到端训练耗时。项链接:https://github.com/deepseek-ai/profile-data行业影响:破解大模型训练瓶颈此次技术发布引发业界强烈关注。专家指出，DualPipe与EPLB的组合创新，直接回应了当前大模型训练的两大挑战:一是随着模型规模指数级增长，传统并行策略的扩展性瓶颈日益凸显;二是混合专家模型的普及使得动态负载均衡成为刚需。某云计算厂商技术负责人评价称:“这些工具将大幅降低千亿级模型训练的硬件门槛，预计可使训练成本下降20%-30%。”DeepSeek CTO在技术文档中强调，此次开源的策略已在其内部多个千亿参数模型训练中验证，未来将持续迭代优化。目前三项技术均已在GitHub开放源码，支持开发者定制化应用于不同硬件环境。随着全球AI竞赛进入“规模决胜”阶段，DeepSeek通过连续四天的关键技术开源，不仅展示了中国AI企业的技术实力，更为行业提供了可复用的基础设施。这场以“开放协作”驱动的技术革新，或将重塑大模型训练的产业生态。
