# Meta AI 推出大概念模型:超越传统语言模型的新突破

**发布日期**: 2024年12月16号 1:28

![新闻图片](https://pic.chinaz.com/picmap/thumb/202304251756308111_1.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13963)

## 内容

近年来，大型语言模型（LLMs）在自然语言处理(NLP)领域取得了显著进展，广泛应用于文本生成、摘要和问答等场景。然而，这些模型依赖于逐字预测的 token 级处理方法，这种方式在理解上下文时存在困难，且往往产生不一致的输出。此外，将 LLMs 扩展到多语言和多模态应用时，计算成本和数据需求也相对较高。为了解决这些问题，Meta AI 提出了一种全新的方法 —— 大概念模型(LCMs)。大概念模型（LCMs）代表了传统 LLM 架构的一次重要转变。它们引入了两个重大创新:首先，LCMs 在一个高维嵌入空间中进行建模，而不是依赖于离散的 tokens。这一嵌入空间被称为 SONAR，旨在支持200多种语言和多种模态，包括文本和语音，提供语言和模态无关的处理能力。其次，LCMs 的设计允许在语义层面上无缝过渡，能够在不同语言和模态之间实现强大的零 - shot 泛化能力。在 LCMs 的核心，存在概念编码器和解码器，这些组件将输入句子映射到 SONAR 的嵌入空间，并将嵌入解码回自然语言或其他模态。这些组件的冻结设计确保了模块化，方便在不重训整个模型的情况下扩展新语言或模态。技术细节方面，LCMs 采用了层次化架构，模仿人类的推理过程，从而提升了长篇内容的一致性，同时能够在不干扰整体上下文的情况下进行局部编辑。通过采用扩散模型，LCMs 在生成过程中表现出色，这些模型基于前面的嵌入预测下一个 SONAR 嵌入。实验中，采用了单塔和双塔两种架构，其中双塔架构在上下文编码和去噪上分开处理，提高了效率。实验结果显示，基于扩散的双塔 LCM 在多个任务中展现了竞争力，如多语言摘要任务中，LCMs 在零 - shot 情况下的表现优于基线模型，证明了它们的适应能力。同时，LCMs 在处理较短序列时也表现出高效性和准确性，相关度量指标的显著提升印证了这一点。Meta AI 的大概念模型为传统 token 级语言模型提供了一种有前途的替代方案，通过高维概念嵌入和模态无关的处理，解决了现有方法的一些关键局限。随着对这一架构研究的深入，LCMs 有望重新定义语言模型的能力，为 AI 驱动的沟通提供更具可扩展性和适应性的方法。项目入口:https://github.com/facebookresearch/large_concept_model划重点:🌟 LCMs 在高维嵌入空间中进行建模，支持200多种语言和多模态。💡 LCMs 采用层次化架构，提升长篇内容的一致性和局部编辑能力。🚀 研究结果显示，LCMs 在多语言摘要等任务中表现优异，具备强大的零 - shot 泛化能力。
