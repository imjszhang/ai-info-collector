# Meta 推出“大型概念模型” LCMs！突破 LLM 局限，引领 AI 语言理解新方向

**发布日期**: 2024年12月16号 6:28

![新闻图片](https://pic.chinaz.com/picmap/thumb/202207271436142427_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13985)

## 内容

大型语言模型 (LLM) 在自然语言处理 (NLP) 领域取得了显著进展，使其在文本生成、摘要和问答等应用中大放异彩。然而，LLM 对令牌级处理（一次预测一个词）的依赖也带来了一些挑战。这种方法与人类的交流方式形成对比，后者通常在更高层次的抽象层面运作，例如句子或想法。令牌级建模在需要长上下文理解的任务中也显得力不从心，并可能产生不一致的输出。此外，将这些模型扩展到多语言和多模态应用中，在计算上成本高昂，且需要大量数据。为了解决这些问题，Meta AI 的研究人员提出了一种新的方法：大型概念模型 (LCM)。大型概念模型：语义理解新范式Meta AI 的大型概念模型 (LCM) 代表了传统 LLM 架构的转变。LCM 引入了两项重大创新：高维嵌入空间建模：LCM 不再对离散令牌进行操作，而是在高维嵌入空间中执行计算。这个空间表示抽象的意义单位，称为概念，对应于句子或话语。这个名为 SONAR 的嵌入空间被设计为与语言和模态无关，支持 200 多种语言和多种模态，包括文本和语音。与语言和模态无关的建模：与绑定到特定语言或模态的模型不同，LCM 在纯粹的语义层面处理和生成内容。这种设计允许在语言和模态之间无缝切换，从而实现强大的零样本泛化。LCM 的核心是概念编码器和解码器，它们将输入句子映射到 SONAR 的嵌入空间，并将嵌入解码回自然语言或其他模态。这些组件是冻结的，确保了模块化，并且易于扩展到新的语言或模态，而无需重新训练整个模型。LCM 的技术细节和优势LCM 引入了几项创新来推进语言建模：分层架构：LCM 采用分层结构，镜像人类的推理过程。这种设计提高了长篇内容的连贯性，并允许局部编辑，而不会破坏更广泛的上下文。基于扩散的生成：扩散模型被认为是 LCM 最有效的设计。这些模型根据前面的嵌入预测下一个 SONAR 嵌入。探索了两种架构：单塔：单个 Transformer 解码器处理上下文编码和去噪。双塔：将上下文编码和去噪分开，为每个任务提供专用组件。可扩展性和效率：与令牌级处理相比，概念级建模减少了序列长度，解决了标准 Transformer 的二次复杂性，并能够更有效地处理长上下文。零样本泛化：LCM 通过利用 SONAR 广泛的多语言和多模态支持，在看不见的语言和模态上表现出强大的零样本泛化能力。搜索和停止标准：基于与 “文档结束” 概念的距离的停止标准的搜索算法，确保连贯和完整的生成，而无需进行微调。实验结果的启示Meta AI 的实验突出了 LCM 的潜力。一个扩展到 70 亿参数的基于扩散的双塔 LCM 在摘要等任务中表现出竞争优势。 主要结果包括：多语言摘要：LCM 在多种语言的零样本摘要中优于基线模型，展示了其适应性。摘要扩展任务：这个新颖的评估任务展示了 LCM 生成具有连贯性和一致性的扩展摘要的能力。效率和准确性：LCM 处理更短的序列比基于令牌的模型更有效率，同时保持了准确性。 研究结果详细说明，诸如互信息和对比准确性等指标显示出显著的改进。总结Meta AI 的大型概念模型为传统的基于令牌的语言模型提供了一种有希望的替代方案。通过利用高维概念嵌入和与模态无关的处理，LCM 解决了现有方法的主要局限性。它们的分层架构提高了连贯性和效率，而其强大的零样本泛化能力则将其适用性扩展到不同的语言和模态。随着对这种架构研究的继续，LCM 有可能重新定义语言模型的能力，为 AI 驱动的通信提供更可扩展和适应性更强的方法。总而言之，Meta 的 LCM 模型代表了 AI 语言理解领域的一项重要突破。 它为我们提供了一种超越传统令牌级建模的新视角，有望在未来的 AI 应用中发挥更大的作用。
