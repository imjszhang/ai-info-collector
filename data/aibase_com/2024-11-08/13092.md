# 研究人员发现一种训练大型语言模型的省力方法 能耗降低30%

**发布日期**: 2024年11月8号 11:01

![新闻图片](https://pic.chinaz.com/picmap/202306131355463905_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/13092)

## 内容

近日，密歇根大学的一项新研究发现，一种训练大型语言模型的省力方法可以在相同的时间内完成，但能耗可降低30%。这种方法可以节省足够的能源，到2026年为110万美国家庭供电。研究人员开发了一款名为 Perseus 的软件工具，通过识别关键路径，即需要最长时间才能完成的一系列子任务。然后，Perseus 会降低非关键路径上的处理器速度，以便它们都能在同一时间完成工作，从而消除不必要的功耗。该团队通过训练 GPT-3、其他三个大型语言模型和一个计算机视觉模型来测试 Perseus。结果表明，Perseus 可以降低 AI 训练的能耗，同时保持相同的训练速度。研究人员表示，这种省力方法对于公平使用人工智能具有重要意义。如果一个国家没有足够的电力来运行大型模型，他们可能需要使用远程服务，或者只能运行较小、准确度较低的模型。这种差距可能会进一步加剧不同社区之间的差距。该研究表明，通过优化 AI 训练方法，可以降低能耗，同时保持相同的训练速度。这对于节省能源和减少碳足迹具有重要意义。
