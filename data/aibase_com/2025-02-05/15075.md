# Anthropic 推出“体质分类器”：成功阻止95% 的模型越狱尝试

**发布日期**: 2025年2月5号 14:13

![新闻图片](https://pic.chinaz.com/picmap/thumb/202307120853038799_0.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15075)

## 内容

人工智能公司 Anthropic 近日宣布开发了一种名为“体质分类器”的新安全方法，旨在保护语言模型免受恶意操纵。该技术专门针对“通用越狱”——一种试图系统性绕过所有安全措施的输入方式，以防止 AI 模型生成有害内容。为了验证这一技术的有效性，Anthropic 进行了一项大规模测试。公司招募了183名参与者，在两个月内尝试突破其防御系统。参与者被要求通过输入特定问题，试图让人工智能模型 Claude3.5回答十个禁止的问题。尽管提供了高达15，000美元的奖金和约3，000小时的测试时间，但没有任何参与者能够完全绕过 Anthropic 的安全措施。从挑战中进步Anthropic 的早期版本“体质分类器”存在两个主要问题:一是将过多无害请求误判为危险请求，二是需要大量计算资源。经过改进，新版分类器显著降低了误判率，并优化了计算效率。然而，自动测试显示，尽管改进后的系统成功阻止了超过95% 的越狱尝试，但仍需额外23.7% 的计算能力来运行。相比之下，未受保护的 Claude 模型允许86% 的越狱尝试通过。基于合成数据的训练“体质分类器”的核心在于使用预定义的规则（称为“宪法”）来区分允许和禁止的内容。系统通过生成多种语言和风格的合成训练示例，训练分类器识别可疑输入。这种方法不仅提高了系统的准确性，还增强了其应对多样化攻击的能力。尽管取得了显著进展，Anthropic 的研究人员承认，该系统并非完美无缺。它可能无法应对所有类型的通用越狱攻击，且未来可能会出现新的攻击方法。因此，Anthropic 建议将“体质分类器”与其他安全措施结合使用，以提供更全面的保护。公开测试与未来展望为进一步测试系统的强度，Anthropic 计划在2025年2月3日至10日期间发布公开演示版本，邀请安全专家尝试破解。测试结果将在后续更新中公布。这一举措不仅展示了 Anthropic 对技术透明度的承诺，也为 AI 安全领域的研究提供了宝贵的数据。Anthropic 的“体质分类器”标志着 AI 模型安全防护的重要进展。随着 AI 技术的快速发展，如何有效防止模型被滥用已成为行业关注的焦点。Anthropic 的创新为这一挑战提供了新的解决方案，同时也为未来的 AI 安全研究指明了方向。
