# DeepSeek开源周第二日：首个面向MoE模型的开源EP通信库

**发布日期**: 2025年2月25号 11:02

![新闻图片](https://upload.chinaz.com/2025/0225/6387607791714831703467365.png)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/15684)

## 内容

Deepseek 公布了开源周第二天的产品，首个面向MoE模型的开源EP通信库，支持实现了混合专家模型训练推理的全栈优化。DeepEP 是一个专为混合专家（MoE）和专家并行(EP)设计的高效通信库。它致力于提供高吞吐量和低延迟的多对多 GPU 内核，通常被称为 MoE 调度和组合。DeepEP 不仅支持 FP8等低精度操作，还与 DeepSeek-V3论文提出的组限制门控算法相一致，优化了不对称域带宽转发的内核，例如将数据从 NVLink 域转发至 RDMA 域。这些内核具有高吞吐量，非常适合于训练和推理预填充任务，并且可以对流处理器的数量进行控制。对于对延迟敏感的推理解码任务，DeepEP 还包括一组低延迟的内核，利用纯 RDMA 以最小化延迟。此外，DeepEP 还引入了一种基于钩子的通信 - 计算重叠方法，不会占用任何流处理器资源。在性能测试中，DeepEP 在 H800和 CX7InfiniBand400Gb/s RDMA 网络卡上进行了多项测试。测试显示，正常内核在内节点和跨节点的带宽表现优异，而低延迟内核则在延迟和带宽方面都达到了预期效果。具体而言，低延迟内核在处理8个专家时的延迟为163微秒，带宽为46GB/s。DeepEP 经过充分测试，主要与 InfiniBand 网络兼容，但理论上也支持在收敛以太网（RoCE）上运行。为了防止不同流量类型之间的干扰，建议在不同的虚拟通道中隔离流量，确保正常内核和低延迟内核之间不会相互影响。DeepEP 是一个为混合专家模型提供高效通信解决方案的重要工具，具有优化性能、降低延迟和灵活配置等显著特点。项目入口:https://x.com/deepseek_ai/status/1894211757604049133划重点:🌟 DeepEP 专为混合专家模型设计，提供高吞吐量和低延迟的通信解决方案。⚙️ 支持多种低精度操作，并优化了数据转发的带宽性能。💡 经过测试与验证，DeepEP 兼容 InfiniBand 网络，适用于不同流量类型的隔离与管理。
