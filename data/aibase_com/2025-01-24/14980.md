# Sakana AI 的 Transformer² 模型突破 LLM 限制，实现动态推理

**发布日期**: 2025年1月24号 10:48

![新闻图片](https://pic.chinaz.com/picmap/thumb/202305251639365380_20.jpg)

**新闻链接**: [点击查看原文](https://www.aibase.com/zh/news/14980)

## 内容

Sakana AI 是一家专注于自然启发算法的人工智能研究实验室，近日推出了一种名为 Transformer² （Transformer-squared） 的创新自适应语言模型。该模型无需昂贵的微调，即可在推理过程中动态学习并适应新任务，这标志着大型语言模型 (LLM) 技术发展的重要一步。Transformer² 的核心创新在于其独特的两步动态权重调整机制。首先，它分析传入的用户请求，理解任务需求;然后，通过数学技巧，利用奇异值分解 （SVD） 将模型权重与任务需求对齐。通过有选择地调整模型权重的关键组件，Transformer² 能够实时优化性能，而无需耗时的重新训练。这与传统的微调方法形成鲜明对比，后者需要在训练后保持参数静态，或者采用低秩自适应 (LoRA) 等方法，仅修改一小部分参数。Transformer 平方训练和推理（来源:arXiv）为了实现动态调整，研究人员采用了奇异值微调 （SVF） 的方法。在训练时，SVF 从模型的 SVD 组件中学习一组被称为 z 向量的技能表示。在推理时，Transformer² 通过分析提示来确定所需技能，然后配置相应的 z 向量，从而实现为每个提示量身定制的响应。测试结果显示，Transformer² 在数学、编码、推理和视觉问答等各种任务中均优于 LoRA 模型，且参数更少。更令人瞩目的是，该模型还具有知识迁移能力，即从一个模型学习到的 z 向量可以应用到另一个模型，从而表明了广泛应用的潜力。Transformer-squared（表中的 SVF）与基础模型和 LoRA 的比较(来源:arXiv)Sakana AI 在其 GitHub 页面上发布了 Transformer² 组件的训练代码，为其他研究人员和开发人员打开了大门。随着企业不断探索 LLM 的应用，推理时定制技术正逐渐成为主流趋势。Transformer² 与 Google 的 Titans 等其他技术一道，正在改变 LLM 的应用方式，使用户能够根据其特定需求动态调整模型，而无需重新训练。这种技术的进步将使 LLM 在更广泛的领域内更加有用和实用。Sakana AI 的研究人员表示，Transformer² 代表了静态人工智能与生命智能之间的桥梁，为高效、个性化和完全集成的人工智能工具奠定了基础。
